---
title: "Ghana Air Quality Analysis"
output: 
  html_document:
    toc: true
    theme: united
date: "2024-06-13"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r packages, echo=FALSE, message=FALSE, warning=FALSE}
### load packages 
library(here) 
library(lubridate) 
library(tictoc)
library(DT)
library(purrr)
library(tidyverse)
library(data.table)
library(sf)
library(viridis)
library(knitr)
library(modelsummary)
library(spdep)
library(gstat)
library(units) 
library(gridExtra)

library(broom)
library(Metrics)
library(kableExtra)

```


```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Source in functions

# source in function that loads each pollution dataset separately to keep data small and prevent R crashes 
source(here("src", "load_pollution_datasets.R"))

# source function that aggregates data by time scale of interest (hourly, daily)
source(here("src", "summarize_pollution_times.R"))

# Source function that creates fleet average vs monitor pollutant reading plot
source(here("src", "compare_fleet_avg_monitor.R"))

# Source function that creates heatmap of pollutant readings
source(here("src", "generate_heatmap.R"))

# Source function to make map of pollution values at each monitor location
source(here("src", "generate_spatial_pollution_map.R"))

# source function to calculate moran i 
source(here("src", "calculate_moran_i.R"))

# source function to prepare data for road regression analysis and plotting
source(here("src", "prep_monitor_road_data.R"))

# source function to plot map of pollutant levels by location with roads 
source(here("src", "generate_spatial_pollution_road_map.R"))

# source function to run regression analysis on pollutant / distance to road
source(here("src", "regress_pollutant_road.R"))

# source function to apply regressions when comparing monitor to fleet average
source(here("src", "compare_fleet_regression.R"))

# source functions to merge in the SD card data for cases when server data is missing 
source(here("src", "merge_sd_data.R"))
source(here("src", "process_multiple_pollutants.R"))

#source functions to create plot that compares individual monitor to golden monitor
source(here("src", "compare_gas_to_reference_functions.R"))

#source functions to create plot that compares individual monitor to golden monitor
source(here("src", "calc_gas_regressions.R"))
```


```{r location data, echo=FALSE, message=FALSE, warning=FALSE}
#LOAD LOCATION DATA ----

# identify the location columns
location_columns <- c("monitor", "geo_lat", "geo_lon", "date")

# read in the monitor location data using fread to handle large dataset
location_data <- fread("/Users/lewiswhite/CHAP_columbia/QuantAQ/ghana_AQ_parent_full.csv", 
                       select = location_columns, 
                       showProgress = TRUE)

# remove missing values and filter to recent monitor locations 
location_complete <- location_data[complete.cases(location_data$geo_lat, location_data$geo_lon), ] %>%
  filter(date > as.Date("2024-03-01"))

# Get unique combinations for each device
monitor_locations <- location_complete %>%
  group_by(monitor) %>%
  distinct(geo_lat, geo_lon) %>%
  
  # ADJUSTMENTS (ASK DJ TO CONFIRM) DUE TO ERRORS IN LONGITUDE
  mutate(geo_lon = case_when(geo_lon == -173058.0000 ~ -1.73058,
                             geo_lon == 1.5990 ~ -1.5990,
                             TRUE ~ geo_lon)) %>%
  filter(geo_lat != 8.05630)

# create spatial feature dataset of monitor points
monitor_points <- st_as_sf(monitor_locations, coords = c("geo_lon", "geo_lat"), crs = st_crs(4326)) %>%
  mutate(mod_pm = str_detect(monitor, "MOD-PM")) %>%
  mutate(monitor_type = case_when(mod_pm == TRUE ~ "MOD-PM",
                                  mod_pm == FALSE ~ "MODULAIR"))



# LOAD IN THE COUNTRY AND REGION MAP DATA ----

country <- st_read(here("gha_admbnda_gss_20210308_SHP", "gha_admbnda_adm0_gss_20210308.shp"), quiet = TRUE)

regions <- st_read(here("gha_admbnda_gss_20210308_SHP", "gha_admbnda_adm1_gss_20210308.shp"), quiet = TRUE) %>%
  rename(region = ADM1_EN)

bono_east <- regions %>% filter(region == "Bono East")

## SPECIFIC TOWN MARKERS 

# just the lat/lon for kintampo
kintampo_sf <- st_as_sf(data.frame(
  location = "Kintampo",
  geo_lon = -1.7296,
  geo_lat = 8.0593
), coords = c("geo_lon", "geo_lat"), crs = st_crs(4326))

# Amoma 
amoma_sf <- st_as_sf(data.frame(
  location = "amoma",
  geo_lon = -1.878, 
  geo_lat = 7.783
), coords = c("geo_lon", "geo_lat"), crs = st_crs(4326))

# Dwenewoho
dwenewoho_sf <- st_as_sf(data.frame(
  location = "dwenewoho",
  geo_lon = -1.8325,
  geo_lat = 7.7403
), coords = c("geo_lon", "geo_lat"), crs = st_crs(4326))


# New Longoro
new_longoro_sf <- st_as_sf(data.frame(
  location = "New Longoro",
  geo_lon = -2.02953,
  geo_lat = 8.14399
), coords = c("geo_lon", "geo_lat"), crs = st_crs(4326))

# Apesika
apesika_sf <- st_as_sf(data.frame(
  location = "Apesika",
  geo_lon = -1.5453,
  geo_lat = 7.99838
), coords = c("geo_lon", "geo_lat"), crs = st_crs(4326))

# Kurawura Akura
kurawura_akura_sf <- st_as_sf(data.frame(
  location = "Kurawura Akura",
  geo_lon = -1.4684,
  geo_lat = 8.7380
), coords = c("geo_lon", "geo_lat"), crs = st_crs(4326))





# LOAD IN THE ROADS DATA ----

# Load roads data and set CRS (assuming it's EPSG:4326)
roads <- st_read(here("hotosm_gha_roads_lines_shp", "hotosm_gha_roads_lines_shp.shp"), quiet = TRUE)
st_crs(roads) <- 4326


# Define the bounding box
bounding_box <- st_bbox(c(xmin = -2.2, ymin = 7.6, xmax = -1.3, ymax = 8.8), crs = st_crs(4326))

# Create a bounding box as an sf object
bbox_sf <- st_as_sfc(bounding_box)

# Filter the roads dataset to include only those within the bounding box
#roads_filtered <- st_intersection(roads, bbox_sf)

#Filter the roads to only bono east
roads_filtered <- st_intersection(roads, bono_east)


# load weather data ----

met_data <- read_rds(here("metall_2023_2024.rds"))




# LOAD POPULATION DATA FROM KHRC

prisma_populations <- readxl::read_xlsx(here("data", "individuals.xlsx")) 

prisma_populations <- prisma_populations %>%
  mutate(village = sub(" \\(.*$", "", village)) %>%
  mutate(village = str_to_title(village)) %>%
  group_by(village) %>%
  summarise(households = sum(households),
            population = sum(population)) %>%
  mutate(
    village = str_to_title(sub(" \\(.*$", "", village)),
    village = case_when(
      village == "Asantekwa" ~ "Asantekwaa",
      village == "Korawura Akura" ~ "Kurawura Akura",
      village == "Soronuase" ~ "Sorunuase",
      TRUE ~ village
    )
  )



monitor_names <- tibble::tribble(
  ~monitor, ~description, ~location,
  "MOD-PM-00887", "Babator", "Babatokuma GH",
  "MOD-PM-00881", "Mo-line", "Kintampo GH",
  "MOD-PM-01052", "Kokuma", "Kokuma GH",
  "MOD-PM-01055", "Krabonso", "Krabonso GH",
  "MOD-PM-00893", "Jato Akura", "Jato Akura GH",
  "MOD-PM-01060", "Kintampo Central College Areas", "Kintampo GH",
  "MOD-PM-00899", "Nante", "Nante GH",
  "MOD-00399", "Dwenewoho", "Dwenewoho GH",
  "MOD-PM-01057", "Sorunuase", "Sorunuase GH",
  "MOD-PM-00877", "Pamdu", "Pamdu GH",
  "MOD-PM-00898", "Anokyekrom", "Anokyekrom GH",
  "MOD-PM-00900", "Kawampe", "Kawampe GH",
  "MOD-PM-00883", "Portor", "Portor GH",
  "MOD-00398", "Kurawura Akura", "Kurawura Akura GH",
  "MOD-PM-00884", "KHRC Residency", "Kintampo GH",
  "MOD-PM-01056", "Gulumpe", "Gulumpe GH",
  "MOD-PM-00879", "Dawadawa", "Dawadawa GH",
  "MOD-PM-00897", "Pramposo", "Pramposo GH",
  "MOD-PM-01053", "Anyima", "Anyima GH",
  "MOD-PM-00880", "Kadelso", "Kadelso GH",
  "MOD-PM-01051", "Weila", "Weila GH",
  "MOD-PM-01058", "Krutakyi", "Krutakyi GH",
  "MOD-PM-00895", "Ntankro", "Kintampo GH",
  "MOD-PM-00889", "Beposo", "Beposo GH",
  "MOD-PM-00882", "Apaaso", "Kintampo GH",
  "MOD-PM-01059", "Paninamisa", "Paninamisa GH",
  "MOD-00397", "Apesika", "Apesika GH",
  "MOD-PM-00878", "Jema Pentecost", "Jema GH",
  "MOD-00401", "New Longoro", "New Longoro GH",
  "MOD-PM-00891", "Habitat", "Kintampo GH",
  "MOD-PM-00885", "Kwabia", "Kwabia GH",
  "MOD-PM-00890", "Akora", "Akora GH",
  "MOD-PM-00876", "Amoma", "Amoma GH",
  "MOD-PM-00888", "Nante Zongo", "Nante Zongo GH",
  "MOD-PM-00894", "Alhassan Akura", "Alhassan Akura GH",
  "MOD-00400", "Kintampo-PTC Area (Magazine)", "Kintampo GH",
  "MOD-PM-01054", "Ampoma", "Ampoma GH",
  "MOD-PM-00896", "Asantekwaa", "Asantekwaa GH",
  "MOD-PM-00886", "Jema Zongo", "Jema GH",
  "MOD-PM-00892", "Atta Akura", "Atta Akura GH"
)

monitor_names <- monitor_names %>%
  mutate(location = gsub(" GH", "", location)) %>%
  mutate(locations = if_else(grepl("Kintampo", location),
                            paste(description, location, sep = ", "),
                            location))


monitor_community_populations <- full_join(monitor_names, prisma_populations, by = c("location" = "village"))

```

```{r pm1 data, echo=FALSE, message=FALSE, warning=FALSE}
# LOAD POLLUTION DATA ----

# Define the pollutants of interest
pollutants <- c("pm1", "pm10", "pm25")

# Define the list to store results
results <- list()

# Load and summarize the data for each pollutant
for (pollutant in pollutants) {
  load_pollution_datasets(pollutant)
  
  colocation_data <- get(paste0(pollutant, "_colocation"))
  community_data <- get(paste0(pollutant, "_community"))
  
  colocation_summary <- summarize_pollution_times(colocation_data, pollutant)
  community_summary <- summarize_pollution_times(community_data, pollutant)
  
  results[[paste0(pollutant, "_colocation_hourly")]] <- colocation_summary$hourly
  results[[paste0(pollutant, "_colocation_daily")]] <- colocation_summary$daily
  results[[paste0(pollutant, "_community_hourly")]] <- community_summary$hourly
  results[[paste0(pollutant, "_community_daily")]] <- community_summary$daily
}

# Access the summarized results
pm1_colocation_hourly <- results$pm1_colocation_hourly
pm1_colocation_daily <- results$pm1_colocation_daily
pm10_colocation_hourly <- results$pm10_colocation_hourly
pm10_colocation_daily <- results$pm10_colocation_daily
pm25_colocation_hourly <- results$pm25_colocation_hourly
pm25_colocation_daily <- results$pm25_colocation_daily
pm1_community_hourly <- results$pm1_community_hourly
pm1_community_daily <- results$pm1_community_daily
pm10_community_hourly <- results$pm10_community_hourly
pm10_community_daily <- results$pm10_community_daily
pm25_community_hourly <- results$pm25_community_hourly
pm25_community_daily <- results$pm25_community_daily



# LOAD IN THE GAS DATA ----

# Define the pollutants of interest
gasses <- c("co", "no", "no2", "o3")

# Define the list to store results
results <- list()

# Load and summarize the data for each gas
for (gas in gasses) {
  load_pollution_datasets(gas) 
  
  colocation_data_name <- paste0(gas, "_colocation")
  community_data_name <- paste0(gas, "_community")
  
  if (exists(colocation_data_name) && exists(community_data_name)) {
    colocation_data <- get(colocation_data_name)
    community_data <- get(community_data_name)
    
    colocation_summary <- summarize_pollution_times(colocation_data, gas)
    community_summary <- summarize_pollution_times(community_data, gas)
    
    results[[paste0(gas, "_colocation_hourly")]] <- colocation_summary$hourly
    results[[paste0(gas, "_colocation_daily")]] <- colocation_summary$daily
    results[[paste0(gas, "_community_hourly")]] <- community_summary$hourly
    results[[paste0(gas, "_community_daily")]] <- community_summary$daily
  } else {
    warning(paste("Data for", gas, "not found."))
  }
}



co_colocation_hourly <- results$co_colocation_hourly %>% 
  mutate(mod_pm = str_detect(monitor, "MOD-PM")) %>% 
  filter(mod_pm == FALSE) %>%
  select(-mod_pm)

co_colocation_daily <- results$co_colocation_daily %>% 
  mutate(mod_pm = str_detect(monitor, "MOD-PM")) %>% 
  filter(mod_pm == FALSE) %>%
  select(-mod_pm)

co_community_hourly <- results$co_community_hourly %>% 
  mutate(mod_pm = str_detect(monitor, "MOD-PM")) %>% 
  filter(mod_pm == FALSE) %>%
  select(-mod_pm)

co_community_daily <- results$co_community_daily %>% 
  mutate(mod_pm = str_detect(monitor, "MOD-PM")) %>% 
  filter(mod_pm == FALSE) %>%
  select(-mod_pm)


no_colocation_hourly <- results$no_colocation_hourly %>% 
  mutate(mod_pm = str_detect(monitor, "MOD-PM")) %>% 
  filter(mod_pm == FALSE) %>%
  select(-mod_pm)

no_colocation_daily <- results$no_colocation_daily %>% 
  mutate(mod_pm = str_detect(monitor, "MOD-PM")) %>% 
  filter(mod_pm == FALSE) %>%
  select(-mod_pm)

no_community_hourly <- results$no_community_hourly %>% 
  mutate(mod_pm = str_detect(monitor, "MOD-PM")) %>% 
  filter(mod_pm == FALSE) %>%
  select(-mod_pm)

no_community_daily <- results$no_community_daily %>% 
  mutate(mod_pm = str_detect(monitor, "MOD-PM")) %>% 
  filter(mod_pm == FALSE) %>%
  select(-mod_pm)

no2_colocation_hourly <- results$no2_colocation_hourly %>% 
  mutate(mod_pm = str_detect(monitor, "MOD-PM")) %>% 
  filter(mod_pm == FALSE) %>%
  select(-mod_pm)

no2_colocation_daily <- results$no2_colocation_daily %>% 
  mutate(mod_pm = str_detect(monitor, "MOD-PM")) %>% 
  filter(mod_pm == FALSE) %>%
  select(-mod_pm)

no2_community_hourly <- results$no2_community_hourly %>% 
  mutate(mod_pm = str_detect(monitor, "MOD-PM")) %>% 
  filter(mod_pm == FALSE) %>%
  select(-mod_pm)

no2_community_daily <- results$no2_community_daily %>% 
  mutate(mod_pm = str_detect(monitor, "MOD-PM")) %>% 
  filter(mod_pm == FALSE) %>%
  select(-mod_pm)


o3_colocation_hourly <- results$o3_colocation_hourly %>% 
  mutate(mod_pm = str_detect(monitor, "MOD-PM")) %>% 
  filter(mod_pm == FALSE) %>%
  select(-mod_pm)

o3_colocation_daily <- results$o3_colocation_daily %>% 
  mutate(mod_pm = str_detect(monitor, "MOD-PM")) %>% 
  filter(mod_pm == FALSE) %>%
  select(-mod_pm)

o3_community_hourly <- results$o3_community_hourly %>% 
  mutate(mod_pm = str_detect(monitor, "MOD-PM")) %>% 
  filter(mod_pm == FALSE) %>%
  select(-mod_pm)

o3_community_daily <- results$o3_community_daily %>% 
  mutate(mod_pm = str_detect(monitor, "MOD-PM")) %>% 
  filter(mod_pm == FALSE) %>%
  select(-mod_pm)
```


## Monitor Locations

```{r monitor locations, echo=FALSE, message=FALSE, warning=FALSE}
# Define the bounding box coordinates
bbox_coords <- matrix(c(-2.2, 7.6, -2.2, 8.8, -1.3, 8.8, -1.3, 7.6, -2.2, 7.6), ncol = 2, byrow = TRUE)
bbox_polygon <- st_polygon(list(bbox_coords))
bbox_sf <- st_sfc(bbox_polygon, crs = st_crs(4326))

# Plot the map with the bounding box
ggplot() +
  # Add Ghana regions
  geom_sf(data = regions, fill = ifelse(regions$region == "Bono East", "#f5e493", "#dadbe0"), color = "black", alpha = 0.5) +
  # Add monitor points with transparency
  geom_sf(data = monitor_points, aes(geometry = geometry), color = "red", alpha = 0.5, size = 2) +
  # Add bounding box
  geom_sf(data = bbox_sf, fill = NA, color = "blue", lwd = 0.5, linetype = "solid") +
  # Add Ghana country boundary
  geom_sf(data = country, fill = NA, color = "black", size = 1) +
  #labs(title = "Monitor Locations in Ghana") +
  theme_bw() +
  theme(
    axis.text = element_text(size = 11), # Increase axis text size
    axis.ticks = element_line(size = 1), # Increase tick mark size
    plot.title = element_text(size = 16) 
  )


ggplot() +
  # Add Ghana regions within the bounding box
  geom_sf(data = regions, fill = ifelse(regions$region == "Bono East", "#f5e493", "#dadbe0"), color = "black", alpha = 0.5) +
  # Add monitor points with transparency within the bounding box
  geom_sf(data = monitor_points, aes(geometry = geometry, color = monitor_type), alpha = 0.5, size = 5) +
  
  # Add Kintampo marker
  geom_sf(data = kintampo_sf, aes(geometry = geometry), shape = 20, size = 3, color = "black") +
  # Add Dwenewoho marker
  geom_sf(data = dwenewoho_sf, aes(geometry = geometry), shape = 20, size = 3, color = "black") +
  # Add new longoro marker
  geom_sf(data = new_longoro_sf, aes(geometry = geometry), shape = 20, size = 3, color = "black") +
  # Add apesika marker
  geom_sf(data = apesika_sf, aes(geometry = geometry), shape = 20, size = 3, color = "black") +
  # Add kurawura akura marker
  geom_sf(data = kurawura_akura_sf, aes(geometry = geometry), shape = 20, size = 3, color = "black") +
  
  # Add Kintamp annotation text
  annotate("text", x = -1.73, y = 8.10, label = "Kintampo", color = "black", size = 5) +
  # Add Dwenewoho annotation text
  annotate("text", x = -1.69, y = 7.74, label = "Dwenewoho", color = "black", size = 5) +
  # Add new longoro annotation text
  annotate("text", x = -2.047, y = 8.214, label = "New Longoro", color = "black", size = 5) +
  # Add apesika annotation text
  annotate("text", x = -1.459, y = 7.96, label = "Apesika", color = "black", size = 5) +
  # Add atta akura annotation text
  annotate("text", x = -1.65, y = 8.75, label = "Kurawura Akura", color = "black", size = 5) +
  
  # Zoom in to the bounding box
  coord_sf(xlim = c(-2.2, -1.3), ylim = c(7.6, 8.8), expand = FALSE) +
  labs(x = "", 
       y = "",
       color = "Monitor Type") +
  theme_bw() +
  theme(
    axis.text = element_text(size = 11), # Increase axis text size
    axis.ticks = element_line(size = 0.5), # Increase tick mark size
    plot.title = element_text(size = 16), # Increase title size
    legend.text = element_text(size = 10), # Increase legend text size
    legend.title = element_text(size = 12) # Increase legend title size
  )




```


# Summarizing Missingness of Server Data

## Colocation Missingness

```{r colocation missingness, echo=FALSE, message=FALSE, warning=FALSE}
# THE MISSING DATA APPEARS TO BE CONSISTENT ACCROSS POLLUTANTS, SO USING PM 1 TO IDENTIFY MISSING VALUES 

# calculate total missingness across all of the monitors and the entire time range of interest 
overall_missingness <- mean(is.na(pm1_colocation$pm1)) * 100

# daily missingness  
daily_missingness <- pm1_colocation %>% 
  group_by(date) %>% 
  summarise(missing_rate = mean(is.na(pm1)) * 100) %>%
  
  filter(date > as.Date("2023-08-15") & date < as.Date("2023-09-21"))

# hourly missingingness
hourly_missingness <- pm1_colocation %>% 
  group_by(hour) %>% 
  summarise(missing_rate = mean(is.na(pm1)) * 100)

#monitor missingness
monitor_missingness <- pm1_colocation %>% 
  group_by(monitor) %>% 
  summarise(missing_rate = mean(is.na(pm1)) * 100) %>%
  mutate(monitor = fct_reorder(monitor, -missing_rate))



## DAILY MISSING DATA PERCENTAGE ----

#aggregated across monitors 
ggplot(daily_missingness, aes(x = date, y = missing_rate)) +
  geom_line() +
  labs(title = "Daily Missing Data Rate Over Time During Monitor Colocation", x = "Date", y = "Missing Data Rate (%)") +
  theme_minimal() +
  geom_hline(yintercept = overall_missingness, color = "red")

#aggregated across monitors, smoothed
ggplot(daily_missingness, aes(x = date, y = missing_rate)) +
  stat_smooth(span = 0.5) +
  labs(title = "Daily Missing Data Rate Over Time During Monitor Colocation", x = "Date", y = "Missing Data Rate (%)") +
  theme_minimal() +
  geom_hline(yintercept = overall_missingness, color = "red")

  


## TOTAL MISSING DATA FOR EACH MONITOR ----

ggplot(monitor_missingness, aes(x = monitor, y = missing_rate)) +
  geom_bar(stat = "identity") +
  labs(title = "Missing Data Rate by Monitor During Colocation", x = "Monitor", y = "Missing Data Rate (%)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) 


# MISSING DATA PERCENTAGE FOR EACH HOUR OF THE DAY

#aggregated by monitor 
ggplot(hourly_missingness, aes(x = hour, y = missing_rate)) +
  geom_line() +
  labs(title = "Hourly Missing Data Rate During Monitor Colocation", x = "Hour of the Day", y = "Missing Data Rate (%)") +
  theme_minimal()

#facet wrap for each monitor 
pm1_colocation %>% 
  group_by(hour, monitor) %>% 
  summarise(missing_rate = mean(is.na(pm1)) * 100) %>%
  ggplot(aes(x = hour, y = missing_rate)) +
  geom_line() +
  facet_wrap(~monitor) +
  labs(y = "Missing data rate (%)",
       x = "Hour of the Day",
       title = "Hourly Missing Data Rate for Each Monitor During Colocation") +
  theme_bw()



## HEATMAP OF MISSINGNESS PER DAY FOR EACH MONITOR ----
pm1_colocation_with_week <- pm1_colocation %>%
  mutate(week = floor_date(date, "week")) # Extract the week


daily_na <- pm1_colocation_with_week %>%
  group_by(monitor, date) %>%
  summarize(na_percent = sum(is.na(pm1)) / n() * 100) %>%
  ungroup() %>%
  mutate(week = floor_date(date, "week"),  # Extract the week again for plotting
         day_of_week = factor(weekdays(date), 
                              levels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")))  # Reorder days of the week
```

```{r colocation missingness heatmap, fig.height=12, fig.width=8, echo=FALSE, message=FALSE, warning=FALSE}
ggplot(daily_na, aes(x = week, y = day_of_week, fill = na_percent)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "lightgreen", high = "darkgreen", na.value = "white") +
  labs(title = "Percentage of Missing PM1 Data by Day During Colocation",
       x = "Week",
       y = "Day of Week",
       fill = "NA Percent") +
  scale_y_discrete(limits = rev(levels(daily_na$day_of_week)),  # Reverse the order to have Sunday at the top
                   breaks = c("Sunday", "Saturday")) +    # Label only specific days
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  facet_wrap(~ monitor, scales = "free_x", ncol = 5)
```


## Community Missingness

```{r missing data, echo=FALSE, message=FALSE, warning=FALSE}
# calculate total missingness across all of the monitors and the entire time range of interest 
overall_missingness <- mean(is.na(pm1_community$pm1)) * 100

# daily missingness  
daily_missingness <- pm1_community %>% 
  group_by(date) %>% 
  summarise(missing_rate = mean(is.na(pm1)) * 100) %>%
  
  filter(date > as.Date("2023-09-25") & date < as.Date("2024-06-11"))

# hourly missingingness
hourly_missingness <- pm1_community %>% 
  group_by(hour) %>% 
  summarise(missing_rate = mean(is.na(pm1)) * 100)

#monitor missingness
monitor_missingness <- pm1_community %>% 
  group_by(monitor) %>% 
  summarise(missing_rate = mean(is.na(pm1)) * 100) %>%
  mutate(monitor = fct_reorder(monitor, -missing_rate))



## DAILY MISSING DATA PERCENTAGE ----

#aggregated across monitors 
ggplot(daily_missingness, aes(x = date, y = missing_rate)) +
  geom_line() +
  labs(title = "Daily Missing Data Rate Over Time During Community Deployment", x = "Date", y = "Missing Data Rate (%)") +
  theme_minimal() +
  geom_hline(yintercept = overall_missingness, color = "red")


ggplot(daily_missingness, aes(x = date, y = missing_rate)) +
  stat_smooth(span = 0.5) +
  labs(title = "Daily Missing Data Rate Over Time During Community Deployment", x = "Date", y = "Missing Data Rate (%)") +
  theme_minimal() +
  geom_hline(yintercept = overall_missingness, color = "red")

  


## TOTAL MISSING DATA FOR EACH MONITOR ----

ggplot(monitor_missingness, aes(x = monitor, y = missing_rate)) +
  geom_bar(stat = "identity") +
  labs(title = "Missing Data Rate by Monitor During Community Deployment", x = "Monitor", y = "Missing Data Rate (%)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) 


# MISSING DATA PERCENTAGE FOR EACH HOUR OF THE DAY

#aggregated by monitor 
ggplot(hourly_missingness, aes(x = hour, y = missing_rate)) +
  geom_line() +
  labs(title = "Hourly Missing Data Rate During Community Deployment", x = "Hour of the Day", y = "Missing Data Rate (%)") +
  theme_minimal()

#facet wrap for each monitor 
pm1_community %>% 
  group_by(hour, monitor) %>% 
  summarise(missing_rate = mean(is.na(pm1)) * 100) %>%
  ggplot(aes(x = hour, y = missing_rate)) +
  geom_line() +
  facet_wrap(~monitor) +
  theme_bw() +
  labs(title = "Hourly Missing Data Rate for Each Monitor During Community Deployment", x = "Hour of the Day", y = "Missing Data Rate (%)") +
  theme_minimal()


# MAP OF MONITOR MISSINGNESS BY LOCATION ----
missing_monitor_location <- left_join(monitor_missingness, monitor_points)

ggplot() +
  # Add Ghana regions within the bounding box
  geom_sf(data = regions, fill = ifelse(regions$region == "Bono East", "#f5e493", "lightblue"), color = "black", alpha = 0.5) +
  
  # Add monitor points with transparency within the bounding box
  geom_sf(data = missing_monitor_location, aes(geometry = geometry, color = missing_rate), alpha = 0.7, size = 5) +

  # Zoom in to the bounding box
  coord_sf(xlim = c(-2.2, -1.3), ylim = c(7.6, 8.8), expand = FALSE) +
  
  # Customize
  labs(title = "Monitor Missing Data and Location",
       x = "", 
       y = "") +
  theme_bw() +
  scale_color_gradient(low = "#0540a1", high = "#eb4c2d", name = "Missing Rate (%)")
```

### Spatial Correlation of Missing Data 
```{r spatial missing data, echo=FALSE, message=FALSE, warning=FALSE}
# Convert to an sf object
missing_monitor_location <- st_as_sf(missing_monitor_location)

coords <- st_coordinates(missing_monitor_location)
nb <- knn2nb(knearneigh(coords, k = 5)) # k-nearest neighbors
listw <- nb2listw(nb, style = "W")
  
# Calculate Moran's I
moran_result <- moran.test(missing_monitor_location$missing_rate, listw)

# Extract relevant values directly
observed_value <- as.numeric(moran_result$estimate["Moran I statistic"])
expected_value <- as.numeric(moran_result$estimate["Expectation"])
variance <- as.numeric(moran_result$estimate["Variance"])
z_score <- as.numeric(moran_result$statistic)
p_value <- as.numeric(moran_result$p.value)

# Create the Moran's I results table
moran_table <- data.frame(
  Test = "Moran's I Statistic",
  `Observed Value` = observed_value,
  `Expected Value` = expected_value,
  `Variance` = variance,
  `Z-score` = z_score,
  `p-value` = p_value
)

moran_table
```

**Interpretation of Moran I Statistic**

-   -1: The variable of interest is perfectly dispersed

-   0: The variable of interest is randomly dispersed

-   1: The variable of interest is perfectly clustered together

```{r missingness heat map, fig.height=16, fig.width=9, echo=FALSE, message=FALSE, warning=FALSE}
## HEATMAP OF MISSINGNESS PER DAY FOR EACH MONITOR ----
pm1_community_with_week <- pm1_community %>%
  mutate(week = floor_date(date, "week")) # Extract the week


daily_na <- pm1_community_with_week %>%
  group_by(monitor, date) %>%
  summarize(na_percent = sum(is.na(pm1)) / n() * 100) %>%
  ungroup() %>%
  mutate(week = floor_date(date, "week"),  # Extract the week again for plotting
         day_of_week = factor(weekdays(date), 
                              levels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")))  # Reorder days of the week

ggplot(daily_na, aes(x = week, y = day_of_week, fill = na_percent)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "lightgreen", high = "darkgreen", na.value = "white") +
  labs(title = "Percentage of Missing PM1 Data by Day During Community Deployment",
       x = "Week",
       y = "Day of Week",
       fill = "NA Percent") +
  scale_y_discrete(limits = rev(levels(daily_na$day_of_week)),  # Reverse the order to have Sunday at the top
                   breaks = c("Sunday", "Saturday")) +    # Label only specific days
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  facet_wrap(~ monitor, scales = "free_x", ncol = 5)
```


# Summarizing Missingness: Merging SD Card Data to Server Data

```{r merging data, echo=FALSE, message=FALSE, warning=FALSE}
## LOAD IN THE DATA THAT CONTAINS SD CARD INFO ----

# List all MOD files with full path
mod_files <- list.files(
  path = "/Users/lewiswhite/CHAP_columbia/QuantAQ/SD_data/MOD", 
  full.names = TRUE, 
  recursive = FALSE, 
  pattern = "MOD-00.*\\.csv"
)

# List all MOD-PM files with full path
mod_pm_files <- list.files(
  path = "/Users/lewiswhite/CHAP_columbia/QuantAQ/SD_data/MOD-PM", 
  full.names = TRUE, 
  recursive = FALSE, 
  pattern = "MOD-PM.*\\.csv"
)

# Function to read, select columns, and add monitor name for MOD files
read_and_select_mod <- function(file) {
  # Extract the monitor name from the file name
  monitor_name <- str_extract(basename(file), "MOD-\\d+")
  
  read_csv(file) %>%
    select(timestamp_iso, co:pm25) %>%
    mutate(monitor = monitor_name)
}

# Function to read, select columns, and add monitor name for MOD-PM files
read_and_select_mod_pm <- function(file) {
  # Extract the monitor name from the file name
  monitor_name <- str_extract(basename(file), "MOD-PM-\\d+")
  
  read_csv(file) %>%
    select(timestamp_iso, pm1:pm25) %>%
    mutate(monitor = monitor_name)
}

# Read all MOD files and combine them
MOD_sd_card <- mod_files %>%
  map_dfr(read_and_select_mod)

# Read all MOD-PM files and combine them
MOD_PM_sd_card <- mod_pm_files %>%
  map_dfr(read_and_select_mod_pm)

MOD_PM_sd_card <- MOD_PM_sd_card %>%
  mutate(co = NA_real_,
         no = NA_real_,
         no2 = NA_real_,
         o3 = NA_real_) %>%
  select(timestamp_iso, co, no, no2, o3, pm1, pm10, pm25, monitor)

full_sd_card = rbind(MOD_sd_card, MOD_PM_sd_card) %>%
  filter(pm10 < 5000)


# write_csv(full_sd_card, here("data", "full_sd_card.csv"))


## MERGE THE SD DATA WITH THE SERVER DATA ---- 

# List of pollutants you want to process
pollutants <- c("pm1", "pm25", "pm10")

raw_data <- list()

for (pollutant in pollutants) {
  load_pollution_datasets(pollutant)
  # Store the raw data
  raw_data[[paste0(pollutant, "_raw")]] <- get(paste0(pollutant, "_raw"))
}


# Process the pollutants
results <- process_multiple_pollutants(pollutants, raw_data, full_sd_card)


## Summary results
pm1_colocation_hourly <- results$pm1$summarized$hourly %>% filter(date >= as.Date("2023-08-16") & date <= as.Date("2023-09-20"))
pm1_colocation_daily <- results$pm1$summarized$daily %>% filter(date >= as.Date("2023-08-16") & date <= as.Date("2023-09-20"))
pm10_colocation_hourly <- results$pm10$summarized$hourly %>% filter(date >= as.Date("2023-08-16") & date <= as.Date("2023-09-20"))
pm10_colocation_daily <- results$pm10$summarized$daily %>% filter(date >= as.Date("2023-08-16") & date <= as.Date("2023-09-20"))
pm25_colocation_hourly <- results$pm25$summarized$hourly %>% filter(date >= as.Date("2023-08-16") & date <= as.Date("2023-09-20"))
pm25_colocation_daily <- results$pm25$summarized$daily %>% filter(date >= as.Date("2023-08-16") & date <= as.Date("2023-09-20"))

```



## caluclating regression for colocation correction 

```{r}

# PM 1 ----
## Full Raw Merged Data
pm1_colocation_merged <- results$pm1$merged %>% filter(date < as.Date("2023-09-26"))


pm1_regression_data <- pm1_colocation_merged %>% 
  group_by(timestamp) %>% 
  summarize(offline_monitors = sum(is.na(pm1))) %>%
  ungroup() %>%
  
  #add the PM data back
  left_join(pm1_colocation_merged) %>%
  
  filter(offline_monitors < 30) %>%
  
  filter(!is.na(pm1)) 

fleet_average_data <- pm1_regression_data %>%
  group_by(timestamp) %>%
  summarise(fleet_average_pm1 = mean(pm1))

pm1_regression_data <- left_join(pm1_regression_data, fleet_average_data)  


pm1_colocation_regression_results <- apply_regression(pm1_regression_data, "pm1", "fleet_average_pm1")





## PM 2.5 ----

pm25_colocation_merged <- results$pm25$merged %>% filter(date < as.Date("2023-09-26"))

pm25_regression_data <- pm25_colocation_merged %>% 
  group_by(timestamp) %>% 
  summarize(offline_monitors = sum(is.na(pm25))) %>%
  ungroup() %>%
  
  #add the PM data back
  left_join(pm25_colocation_merged) %>%
  
  filter(offline_monitors < 30) %>%
  
  filter(!is.na(pm25)) 

fleet_average_data <- pm25_regression_data %>%
  group_by(timestamp) %>%
  summarise(fleet_average_pm25 = mean(pm25))

pm25_regression_data <- left_join(pm25_regression_data, fleet_average_data)  


## PLOTTING pm25 regression data

pm25_reg_data_no_outliers <- pm25_regression_data %>% filter(pm25 < 250, fleet_average_pm25 < 250)

ggplot(pm25_reg_data_no_outliers, aes(x = fleet_average_pm25, y = pm25)) +
    geom_point(alpha = 0.1) +
    facet_wrap(~monitor, ncol = 5) +
    theme_bw() +
    labs(x = "Fleet Average PM 2.5",
         y = "Monitor PM 2.5") +
    scale_x_continuous(breaks = c(0, 100, 200)) +  # Set x-axis ticks
    scale_y_continuous(breaks = c(0, 100, 200)) +  # Set y-axis ticks
    theme(
      axis.title = element_text(size = 14),   # Increase axis titles size
      axis.text = element_text(size = 11),    # Increase axis labels size
      strip.text = element_text(size = 12)    # Increase facet group text size
    )





pm25_colocation_regression_results <- apply_regression(pm25_regression_data, "pm25", "fleet_average_pm25")

pdf_table <- pm25_colocation_regression_results %>%
  kbl(caption = "PM2.5 Colocation Regression Results",
      col.names = c("Monitor", "N", "Slope", "Intercept", "RMSE", "AME"),
      format = "html", 
      align = c("l", "r", "r", "r", "r", "r")) %>%
  kable_classic(full_width = F, html_font = "Times New Roman") %>%
  add_header_above(c(" " = 2, "Regression Parameters" = 2, "Error Metrics" = 2)) %>%
  column_spec(1, bold = T) %>%
  row_spec(0, bold = T) %>%
  footnote(general = "N = number of observations, RMSE = Root Mean Square Error, AME = Absolute Mean Error")

save_kable(pdf_table, file = "pm25_colocation_regression_results.pdf")



## PM 10 ----


pm10_colocation_merged <- results$pm10$merged %>% filter(date < as.Date("2023-09-26"))


pm10_regression_data <- pm10_colocation_merged %>% 
  group_by(timestamp) %>% 
  summarize(offline_monitors = sum(is.na(pm10))) %>%
  ungroup() %>%
  
  #add the PM data back
  left_join(pm10_colocation_merged) %>%
  
  filter(offline_monitors < 30) %>%
  
  filter(!is.na(pm10)) 

fleet_average_data <- pm10_regression_data %>%
  group_by(timestamp) %>%
  summarise(fleet_average_pm10 = mean(pm10))

pm10_regression_data <- left_join(pm10_regression_data, fleet_average_data)  


pm10_colocation_regression_results <- apply_regression(pm10_regression_data, "pm10", "fleet_average_pm10")


```

### applying colocation correction 

```{r}


pm1_community_merged <- results$pm1$merged %>% filter(date >= as.Date("2023-09-26"))

pm1_corrected <- pm1_community_merged %>%
  left_join(pm1_colocation_regression_results, by = "monitor") %>%
  mutate(corrected_pm1 = (pm1 - intercept) / slope) %>%
  select(monitor, timestamp, date, hour, corrected_pm1, source) %>%
  rename(pm1 = corrected_pm1)




pm25_community_merged <- results$pm25$merged %>% filter(date >= as.Date("2023-09-26"))

pm25_corrected <- pm25_community_merged %>%
  left_join(pm25_colocation_regression_results, by = "monitor") %>%
  mutate(corrected_pm25 = (pm25 - intercept) / slope) %>%
  select(monitor, timestamp, date, hour, corrected_pm25, source) %>%
  rename(pm25 = corrected_pm25)




pm10_community_merged <- results$pm10$merged %>% filter(date >= as.Date("2023-09-26"))

pm10_corrected <- pm10_community_merged %>%
  left_join(pm10_colocation_regression_results, by = "monitor") %>%
  mutate(corrected_pm10 = (pm10 - intercept) / slope) %>%
  select(monitor, timestamp, date, hour, corrected_pm10, source) %>%
  rename(pm10 = corrected_pm10)



summarized_pm1 <- summarize_pollution_times(pm1_corrected, "pm1")
summarized_pm25 <- summarize_pollution_times(pm25_corrected, "pm25")
summarized_pm10 <- summarize_pollution_times(pm10_corrected, "pm10")

pm1_community_daily <- summarized_pm1$hourly
pm1_community_daily <- summarized_pm1$daily

pm25_community_daily <- summarized_pm25$hourly
pm25_community_daily <- summarized_pm25$daily

pm10_community_daily <- summarized_pm10$hourly
pm10_community_daily <- summarized_pm10$daily
```



```{r}
test <- pm25_corrected %>%
  group_by(monitor, source) %>%
  summarise(count = n()) %>%
  ungroup() %>%
  group_by(monitor) %>%
  mutate(percentage = count / sum(count) * 100) %>%
  ungroup() %>%
  filter(!is.na(source)) %>%
  mutate(source = factor(source, levels = c("sd_card", "cloud")))

monitor_order <- test %>%
  filter(source %in% c("cloud", "sd_card")) %>%
  group_by(monitor) %>%
  summarise(total = sum(percentage)) %>%
  arrange(desc(total)) %>%
  pull(monitor)

test <- test %>%
  mutate(monitor = factor(monitor, levels = monitor_order))

ggplot(test, aes(x = monitor, y = percentage, fill = source)) +
  geom_bar(stat = "identity") +
  labs(title = "",
       x = "Monitor",
       y = "Percentage of Data Recorded",
       fill = "Data Source") +  # Custom legend title
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        axis.title.x = element_text(size = 14),  # Increase x-axis title size
        axis.title.y = element_text(size = 14),  # Increase y-axis title size
        legend.text = element_text(size = 12),   # Increase legend text size
        legend.title = element_text(size = 14)) +  # Increase legend title size
  scale_y_continuous(labels = scales::percent_format(scale = 1), limits = c(0, 100)) +
  scale_fill_manual(values = c("cloud" = "#2599db", "sd_card" = "#b54033"),
                    labels = c("cloud" = "Cloud", "sd_card" = "SD Card"))





```



# Pollutant Trends

## Grand Average plots 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
daily_pollutants <- left_join(pm1_community_daily, pm25_community_daily) %>% 
  left_join(pm10_community_daily)

daily_pollutants$date <- as.Date(daily_pollutants$date)


daily_pollutants_long <- daily_pollutants %>%
  group_by(date) %>%
  summarise('Mean PM 1' = mean(mean_pm1, na.rm = TRUE),
            'Mean PM 2.5' = mean(mean_pm25, na.rm = TRUE),
            'Mean PM 10' = mean(mean_pm10, na.rm = TRUE)) %>%
  ungroup() %>%
  pivot_longer(cols = c("Mean PM 1", "Mean PM 2.5", "Mean PM 10"), names_to = "pollutant", values_to = "measurement")

# Calculate the mean values for each pollutant
mean_values <- daily_pollutants_long %>%
  group_by(pollutant) %>%
  summarise(mean_measurement = signif(mean(measurement, na.rm = TRUE), 3))

# Merge the mean values back into the original data frame for plotting
daily_pollutants_long <- daily_pollutants_long %>%
  left_join(mean_values, by = "pollutant")


# Convert the 'pollutant' column to a factor with the desired order
daily_pollutants_long$pollutant <- factor(daily_pollutants_long$pollutant, levels = c("Mean PM 1", "Mean PM 2.5", "Mean PM 10"))


daily_pollutants_long %>%
  ggplot(aes(x = date, y = measurement)) +
  geom_line() +
  geom_hline(aes(yintercept = mean_measurement), color = "#cc1414", linetype = "dashed") +
  geom_text(data = mean_values, aes(x = max(daily_pollutants_long$date), y = mean_measurement, label = paste0(pollutant, ": ", round(mean_measurement, 2), " µg/m³")),
            color = "#cc1414", hjust = 1, vjust = -0.9, size = 4.5) +
  theme_bw() +
  labs(x = "Date",
       y = "Pollutant Measurement (µg/m³)") +
  facet_grid(factor(pollutant, levels = c("Mean PM 1", "Mean PM 2.5", "Mean PM 10" )) ~ ., scales = "free_y") +
  theme(
    axis.title = element_text(size = 14),   # Increase axis titles size
    axis.text = element_text(size = 12),    # Increase axis labels size
    strip.text = element_text(size = 12)  # Increase facet group text size
  )





# Define AQI levels with ordered factors
aqi_levels <- data.frame(
  ymin = c(0, 9.1, 35.5, 55.5, 125.5, 225.5),
  ymax = c(9, 35.4, 55.4, 125.4, 225.4, 450),
  fill = factor(c("Good", "Moderate", "Unhealthy for Sensitive Groups", 
                  "Unhealthy", "Very Unhealthy", "Hazardous"), 
                levels = c("Good", "Moderate", "Unhealthy for Sensitive Groups", 
                           "Unhealthy", "Very Unhealthy", "Hazardous")),
  color = c("green", "yellow", "orange", "red", "purple", "maroon")
)

# Plot with legend inside the graph
daily_pollutants_long %>%
  filter(pollutant == "Mean PM 2.5") %>% 
  ggplot(aes(x = date, y = measurement)) +
  # Add background rectangles for AQI levels
  geom_rect(data = aqi_levels, 
            aes(xmin = min(daily_pollutants_long$date), 
                xmax = max(daily_pollutants_long$date), 
                ymin = ymin, 
                ymax = ymax, 
                fill = fill), 
            color = NA, alpha = 0.2, inherit.aes = FALSE) +
  # Add the PM 2.5 line
  geom_line(color = "black") +
  theme_bw() +
  labs(x = "Date",
       y = bquote("Fleet Mean " ~ PM[2.5] ~ (µg/m^3)),
       fill = "US-EPA AQI Risk Categories") +  # Add legend title for the AQI levels
  theme(
    axis.title = element_text(size = 14),   # Increase axis titles size
    axis.text = element_text(size = 12),    # Increase axis labels size
    strip.text = element_text(size = 12),    # Increase facet group text size
    legend.position = c(0.77, 0.73),  # Position legend inside the graph (top right)
    legend.background = element_rect(fill = rgb(1, 1, 1, alpha = 0.7), color = "black"),
    legend.title = element_text(size = 12),  # Adjust legend title size
    legend.text = element_text(size = 10)    # Adjust legend text size
  ) +
  scale_fill_manual(values = aqi_levels$color) +
  guides(fill = guide_legend(reverse = TRUE))  # Reverse the order of the legend


```



```{r}
# First, categorize the measurement values into AQI levels
daily_pollutants_long <- daily_pollutants_long %>%
  mutate(AQI_Category = cut(measurement, 
                            breaks = c(-Inf, 9, 35.4, 55.4, 125.4, 225.4, Inf), 
                            labels = c("Good", "Moderate", "Unhealthy for Sensitive Groups", 
                                       "Unhealthy", "Very Unhealthy", "Hazardous"),
                            right = FALSE))

# Plot with vertical bands and legend on the side
ggplot(daily_pollutants_long %>% filter(pollutant == "Mean PM 2.5"), 
       aes(x = date, y = measurement)) +
  # Add background tiles for AQI levels
  geom_tile(aes(fill = AQI_Category), height = Inf, alpha = 0.3) +
  # Add the PM 2.5 line
  geom_line(color = "black") +
  theme_bw() +
  labs(x = "Date",
       y = bquote("Fleet Mean " ~ PM[2.5] ~ (µg/m^3)),
       fill = "US-EPA AQI Risk Categories") +  # Add legend title for the AQI levels
  theme(
    axis.title = element_text(size = 14),   # Increase axis titles size
    axis.text = element_text(size = 12),    # Increase axis labels size
    strip.text = element_text(size = 12),    # Increase facet group text size
    legend.position = "right",  # Position legend on the right side
    legend.title = element_text(size = 12),  # Adjust legend title size
    legend.text = element_text(size = 10)    # Adjust legend text size
  ) +
  scale_fill_manual(values = aqi_levels$color) +
  guides(fill = guide_legend(reverse = TRUE))  # Reverse the order of the legend

```


pm25_aqi_time_series_plot
## PM 2.5 Colocation 

### Data Quality Check: Comparing PM 2.5 From Individual Monitors to Fleet Average

```{r colocation monitor fleet avg 25, fig.height=10, fig.width=7, echo=FALSE, message=FALSE, warning=FALSE}
# hourly comparison
compare_fleet_avg_monitor(pm25_colocation_hourly, "pm25", "hourly")

# Daily comparison
compare_fleet_avg_monitor(pm25_colocation_daily, "pm25", "daily")
```

#### PM 2.5 Hourly: Individual Monitor Regressed on Fleet Average
**Note: fleet average required at minimum 5 readings to be included in regression)**
```{r, echo=FALSE, message=FALSE, warning=FALSE}
pm25_hourly_regression_data <- pm25_colocation_hourly %>% 
  group_by(date, hour) %>% 
  summarize(offline_monitors = sum(is.na(mean_pm25))) %>%
  ungroup() %>%
  
  #add the PM data back
  left_join(pm25_colocation_hourly) %>%
  
  filter(offline_monitors < 35) %>%
  
  filter(!is.na(mean_pm25))


pm25_colocation_regression_results <- apply_regression(pm25_hourly_regression_data, "mean_pm25", "fleet_average_pm25")

datatable(pm25_colocation_regression_results)
```

#### PM 2.5 Daily: Individual Monitor Regressed on Fleet Average
**Note: fleet average required at minimum 5 readings to be included in regression)**
```{r, echo=FALSE, message=FALSE, warning=FALSE}
pm25_daily_regression_data <- pm25_colocation_daily %>% 
  group_by(date) %>% 
  summarize(offline_monitors = sum(is.na(mean_pm25))) %>%
  ungroup() %>%
  
  #add the PM data back
  left_join(pm25_colocation_daily) %>%
  
  filter(offline_monitors < 35) %>%
  
  filter(!is.na(mean_pm25))


pm25_colocation_regression_results <- apply_regression(pm25_daily_regression_data, "mean_pm25", "fleet_average_pm25")

datatable(pm25_colocation_regression_results)
```

### PM 2.5 Colocation MOD vs MOD-PM measures to check consistency

```{r, echo=FALSE, message=FALSE, warning=FALSE}
mod_pm_pm25 <- pm25_colocation_hourly %>%
  mutate(mod_pm = str_detect(monitor, "MOD-PM")) %>%
  group_by(mod_pm, date, hour) %>%
  summarize(mean_hourly_pm = mean(mean_pm25, na.rm = TRUE)) %>%
  ungroup() %>%
  filter(mod_pm == TRUE)

mod_pm25 <- pm25_colocation_hourly %>%
  mutate(mod_pm = str_detect(monitor, "MOD-PM")) %>%
  group_by(mod_pm, date, hour) %>%
  summarize(mean_hourly_pm = mean(mean_pm25, na.rm = TRUE)) %>%
  ungroup() %>%
  filter(mod_pm == FALSE)

# Extract the mean_hourly_pm vectors
MOD_PM_pm25_means <- mod_pm_pm25$mean_hourly_pm
MOD_pm25_means <- mod_pm25$mean_hourly_pm

# Perform t-test
t_test_result <- t.test(MOD_PM_pm25_means, MOD_pm25_means)

# Print the result
print(t_test_result)
```


### PM 2.5 Trend Heatmap

```{r colocation pm25 trends, echo=FALSE, message=FALSE, warning=FALSE}
generate_heatmap(pm25_colocation_hourly, "pm25")
```

## PM 2.5 Community Deployment

### Checking Individual Monitor to Fleet Average

```{r monitor vs fleet avg 25, fig.height=10, fig.width=7, echo=FALSE, message=FALSE, warning=FALSE}
# hourly comparison
compare_fleet_avg_monitor(pm25_community_hourly, "pm25", "hourly")

# Daily comparison
compare_fleet_avg_monitor(pm25_community_daily, "pm25", "daily")
```

#### PM 2.5 Hourly: Individual Monitor Regressed on Fleet Average
**Note: fleet average required at minimum 5 readings to be included in regression)**
```{r, echo=FALSE, message=FALSE, warning=FALSE}
pm25_hourly_regression_data <- pm25_community_hourly %>% 
  group_by(date, hour) %>% 
  summarize(offline_monitors = sum(is.na(mean_pm25))) %>%
  ungroup() %>%
  
  #add the PM data back
  left_join(pm25_community_hourly) %>%
  
  filter(offline_monitors < 35) %>%
  
  filter(!is.na(mean_pm25))


pm25_colocation_regression_results <- apply_regression(pm25_hourly_regression_data, "mean_pm25", "fleet_average_pm25")

datatable(pm25_colocation_regression_results)
```

#### PM 2.5 Daily: Individual Monitor Regressed on Fleet Average
**Note: fleet average required at minimum 5 readings to be included in regression)**
```{r, echo=FALSE, message=FALSE, warning=FALSE}
pm25_daily_regression_data <- pm25_community_daily %>% 
  group_by(date) %>% 
  summarize(offline_monitors = sum(is.na(mean_pm25))) %>%
  ungroup() %>%
  
  #add the PM data back
  left_join(pm25_community_daily) %>%
  
  filter(offline_monitors < 35) %>%
  
  filter(!is.na(mean_pm25))


pm25_colocation_regression_results <- apply_regression(pm25_daily_regression_data, "mean_pm25", "fleet_average_pm25")

datatable(pm25_colocation_regression_results)
```


### PM 2.5 Trends

```{r pm25 timelines, echo=FALSE, message=FALSE, warning=FALSE}
daily_pm25_all_devices <- pm25_community %>%
  group_by(date, monitor) %>%
  summarize(mean_pm25 = mean(pm25, na.rm = TRUE)) %>%
  ggplot(aes(x = date, y = mean_pm25, group = monitor)) +
  geom_line(color = "black", alpha = 0.2) +
  theme_minimal() +
  labs(y = "Mean PM 2.5",
       title = "Average PM 2.5 Accross All Devices")

daily_cooktime_pm25_all_devices <- pm25_community %>%
  filter(hour >= 16 & hour < 20) %>%
  group_by(monitor, date) %>%
  summarise(mean_pm25 = mean(pm25, na.rm = TRUE)) %>%
  ggplot(aes(x = date, y = mean_pm25, group = monitor)) +
  geom_line(color = "black", alpha = 0.4) +
  theme_minimal() +
  labs(y = "Mean PM 2.5",
       title = "Daily Average PM 2.5 for Each Monitor During Primary Cooking Hours (4-8)") 

grid.arrange(daily_pm25_all_devices, daily_cooktime_pm25_all_devices)

daily_pm25_timeline <- pm25_community %>%
  group_by(date) %>%
  summarize(mean_pm25 = mean(pm25, na.rm = TRUE)) %>%
  ggplot(aes(x = date, y = mean_pm25)) +
  geom_line() +
  theme_minimal() +
  labs(y = "Mean PM 2.5",
       title = "Average PM 2.5 Accross All Devices")

daily_cooktime_pm25_timeline <- pm25_community %>%
  filter(hour >= 16 & hour < 20) %>%
  group_by(date) %>%
  summarize(mean_pm25 = mean(pm25, na.rm = TRUE)) %>%
  ggplot(aes(x = date, y = mean_pm25)) +
  geom_line() +
  theme_minimal() +
  labs(y = "Mean PM 2.5",
       title = "Mean PM 2.5 Accross All Devices During Cooking Hours (4-8)")

grid.arrange(daily_pm25_timeline, daily_cooktime_pm25_timeline)
```

### PM 2.5 Heatmap

```{r pm25 trends, echo=FALSE, message=FALSE, warning=FALSE}
generate_heatmap(pm25_community_hourly, "pm25")
```



### Harmattan vs non-Harmattan

```{r}
harmattan_start <- as.Date("2023-12-01")
harmattan_end <- as.Date("2024-03-01")


pm25_community_hourly_seasonality <- pm25_community_hourly %>% 
  mutate(harmattan = ifelse(date >= harmattan_start & date <= harmattan_end, "Harmattan", "Not Harmattan")) %>%
  mutate(cooking_hour = ifelse(hour >= 16 & hour <= 19, "Cooking Hour", # 4pm - 8pm
                               ifelse(hour == 3, "Not Cooking Hour", NA))) 


# HARMATTAN VS NON HARMATTAN COMP
# harmattan only
harmattan_pm25_hourly <- pm25_community_hourly_seasonality %>% filter(date >= harmattan_start & date <= harmattan_end) %>% 
  filter(!is.na(hour)) 

# not harmattan only
not_harmattan_pm25_hourly <- pm25_community_hourly_seasonality %>% filter(date < harmattan_start | date > harmattan_end) %>%
  filter(!is.na(hour)) 

mean(harmattan_pm25_hourly$mean_pm25, na.rm = TRUE) 
sd(harmattan_pm25_hourly$mean_pm25, na.rm = TRUE) 

mean(not_harmattan_pm25_hourly$mean_pm25, na.rm = TRUE) 
sd(not_harmattan_pm25_hourly$mean_pm25, na.rm = TRUE) 


# COOKTIME VS NOT COOKTIME COMP
# cooktime only

cooktime_pm25_hourly <- pm25_community_hourly_seasonality %>% filter(cooking_hour == "Cooking Hour") %>% 
  filter(!is.na(hour)) 

not_cooktime_pm25_hourly <- pm25_community_hourly_seasonality %>% filter(cooking_hour == "Not Cooking Hour") %>% 
  filter(!is.na(hour)) 

mean(cooktime_pm25_hourly$mean_pm25, na.rm = TRUE) 
sd(cooktime_pm25_hourly$mean_pm25, na.rm = TRUE) 

mean(not_cooktime_pm25_hourly$mean_pm25, na.rm = TRUE)  
sd(not_cooktime_pm25_hourly$mean_pm25, na.rm = TRUE) 



# COOKTIME VS NOT COOKTIME DURING HARMATTAN

mean(harmattan_pm25_hourly$mean_pm25[harmattan_pm25_hourly$cooking_hour == "Cooking Hour"], na.rm = TRUE)
sd(harmattan_pm25_hourly$mean_pm25[harmattan_pm25_hourly$cooking_hour == "Cooking Hour"], na.rm = TRUE)

mean(harmattan_pm25_hourly$mean_pm25[harmattan_pm25_hourly$cooking_hour == "Not Cooking Hour"], na.rm = TRUE)
sd(harmattan_pm25_hourly$mean_pm25[harmattan_pm25_hourly$cooking_hour == "Not Cooking Hour"], na.rm = TRUE)


# COOKTIME VS NOT COOKTIME NOT DURING HARMATTAN

mean(not_harmattan_pm25_hourly$mean_pm25[not_harmattan_pm25_hourly$cooking_hour == "Cooking Hour"], na.rm = TRUE)
sd(not_harmattan_pm25_hourly$mean_pm25[not_harmattan_pm25_hourly$cooking_hour == "Cooking Hour"], na.rm = TRUE)

mean(not_harmattan_pm25_hourly$mean_pm25[not_harmattan_pm25_hourly$cooking_hour == "Not Cooking Hour"], na.rm = TRUE)
sd(not_harmattan_pm25_hourly$mean_pm25[not_harmattan_pm25_hourly$cooking_hour == "Not Cooking Hour"], na.rm = TRUE)




harmattan_t_test <- t.test(mean_pm25 ~ harmattan, data = pm25_community_hourly_seasonality)
cooking_t_test <- t.test(mean_pm25 ~ cooking_hour, data = pm25_community_hourly_seasonality)
cooking_t_test_harmattan <- t.test(mean_pm25 ~ cooking_hour, data = harmattan_pm25_hourly)
cooking_t_test_not_harmattan <- t.test(mean_pm25 ~ cooking_hour, data = not_harmattan_pm25_hourly)





base_plot <- pm25_community_hourly %>% 
  mutate(harmattan = ifelse(date >= harmattan_start & date <= harmattan_end, "Harmattan", "Not Harmattan")) %>%
  mutate(cooking_hour = ifelse(hour >= 15 & hour <= 18, "Cooking Hour", "Not Cooking Hour")) %>%
  filter(!is.na(hour)) %>%
  ggplot(aes(x = harmattan, y = mean_pm25, fill = cooking_hour)) +
  geom_boxplot() +
  theme_classic() +
  labs(y = "PM 2.5 Concentration", 
       x = "Season",
       fill = "Time") +
  theme(legend.position = "right") +
  scale_fill_manual(values = c("Cooking Hour" = "goldenrod", "Not Cooking Hour" = "#52b3eb"))
  

base_plot +
  geom_segment(aes(x = 0.8, xend = 1.2, y = 550, yend = 550), color = "black") +  # Harmattan Cooking Hour vs Not Cooking Hour
  annotate("text", x = 1, y = 560, label = "***") +
  geom_segment(aes(x = 1.8, xend = 2.2, y = 370, yend = 370), color = "black") +  # Not Harmattan Cooking Hour vs Not Cooking Hour
  annotate("text", x = 2, y = 380, label = "***") +
  geom_segment(aes(x = 1, xend = 2, y = 580, yend = 580), color = "black") +  # Harmattan vs Not Harmattan
  annotate("text", x = 1.5, y = 590, label = "***") +
  theme(
    axis.text = element_text(size = 11), # Increase axis text size
    axis.ticks = element_line(size = 0.7), # Increase tick mark size
    legend.text = element_text(size = 10), # Increase legend text size
    legend.title = element_text(size = 12), # Increase legend title size
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
  )
  



base_plot <- pm25_community_hourly %>% 
  mutate(harmattan = ifelse(date >= harmattan_start & date <= harmattan_end, "Harmattan", "Not Harmattan")) %>%
  mutate(
    cooking_period = factor(case_when(
      hour >= 15 & hour <= 18 ~ "Active Cooking Hours",
      (hour < 15 | hour > 18) & hour != 3 ~ "Non-Active Cooking Hours",
      hour == 3 ~ "Control Hour"
    ), levels = c("Active Cooking Hours", "Non-Active Cooking Hours", "Control Hour"))
  ) %>%
  filter(!is.na(hour)) %>%
  ggplot(aes(x = harmattan, y = mean_pm25, fill = cooking_period)) +
  geom_boxplot() +
  theme_classic() +
  labs(y = "PM 2.5 Concentration", 
       x = "Season",
       fill = "Cooking\nPeriod") +  # Change legend title and add line breaks
  theme(legend.position = "right") +
  scale_fill_manual(values = c(
    "Active Cooking Hours" = "goldenrod", 
    "Non-Active Cooking Hours" = "#52b3eb", 
    "Control Hour" = "#b672e0"
  ),
  labels = c("Evening Cooking Hours (4pm-8pm)", "Other Hours (Excluding Evening Cooking \nand Control Hours)", "Control Hour (3am)"))  # Add line breaks in legend labels


base_plot +
  geom_segment(aes(x = 0.72, xend = 0.98, y = 545, yend = 545), color = "black") +  # Harmattan Cooking Hour vs Not Cooking Hour
  annotate("text", x = 0.85, y = 550, label = "***") +
  
  geom_segment(aes(x = 1.02, xend = 1.28, y = 545, yend = 545), color = "black") +  # Harmattan Cooking Hour vs Not Cooking Hour
  annotate("text", x = 1.15, y = 550, label = "***") +
  
  geom_segment(aes(x = 0.72, xend = 1.27, y = 565, yend = 565), color = "black") +  # Harmattan Cooking Hour vs Not Cooking Hour
  annotate("text", x = 1, y = 570, label = "***") +
  
  geom_segment(aes(x = 1.72, xend = 1.98, y = 360, yend = 360), color = "black") +  # Not Harmattan Cooking Hour vs Not Cooking Hour
  annotate("text", x = 1.85, y = 365, label = "***") +
  
  geom_segment(aes(x = 2.02, xend = 2.28, y = 360, yend = 360), color = "black") +  # Not Harmattan Cooking Hour vs Not Cooking Hour
  annotate("text", x = 2.15, y = 365, label = "***") +
  
  geom_segment(aes(x = 1.72, xend = 2.27, y = 380, yend = 380), color = "black") +  # Not Harmattan Cooking Hour vs Not Cooking Hour
  annotate("text", x = 2, y = 385, label = "***") +
  
  geom_segment(aes(x = 1, xend = 2, y = 590, yend = 590), color = "black") +  # Harmattan vs Not Harmattan
  annotate("text", x = 1.5, y = 600, label = "***") +
  
  labs(caption ="Comparisons and their significance are denoted by the horizontal lines, where the three stars indicate p < 0.001.") +
  
  theme(
    axis.text = element_text(size = 11), # Increase axis text size
    axis.ticks = element_line(size = 0.7), # Increase tick mark size
    legend.text = element_text(size = 10), # Increase legend text size
    legend.title = element_text(size = 12), # Increase legend title size
    plot.caption = element_text(size = 11),
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
  )




pm25_community_hourly_seasonality <- pm25_community_hourly %>% 
  mutate(harmattan = ifelse(date >= harmattan_start & date <= harmattan_end, "Harmattan", "Not Harmattan")) %>%
  mutate(
    cooking_period = factor(case_when(
      hour >= 15 & hour <= 18 ~ "Active Cooking Hours",
      (hour < 15 | hour > 18) & hour != 3 ~ "Non-Active Cooking Hours",
      hour == 3 ~ "Control Hour"
    ), levels = c("Active Cooking Hours", "Non-Active Cooking Hours", "Control Hour"))
  )

# harmattan only
harmattan_pm25_hourly <- pm25_community_hourly_seasonality %>% filter(date >= harmattan_start & date <= harmattan_end) %>% 
  filter(!is.na(hour)) 

# not harmattan only
not_harmattan_pm25_hourly <- pm25_community_hourly_seasonality %>% filter(date < harmattan_start | date > harmattan_end) %>%
  filter(!is.na(hour)) 


anova_result <- aov(mean_pm25 ~ cooking_period, data = pm25_community_hourly_seasonality)
summary(anova_result)

# Tukey's Honest Significant Difference test
tukey_result <- TukeyHSD(anova_result)
print(tukey_result)



anova_result <- aov(mean_pm25 ~ cooking_period, data = harmattan_pm25_hourly)
summary(anova_result)

# Tukey's Honest Significant Difference test
tukey_result <- TukeyHSD(anova_result)
print(tukey_result)


anova_result <- aov(mean_pm25 ~ cooking_period, data = not_harmattan_pm25_hourly)
summary(anova_result)

# Tukey's Honest Significant Difference test
tukey_result <- TukeyHSD(anova_result)
print(tukey_result)






# COOKTIME VS NOT COOKTIME COMP
# cooktime only

cooktime_pm25_hourly <- pm25_community_hourly_seasonality %>% filter(cooking_period == "Active Cooking Hours") %>% 
  filter(!is.na(hour)) 

not_cooktime_pm25_hourly <- pm25_community_hourly_seasonality %>% filter(cooking_period == "Non-Active Cooking Hours") %>% 
  filter(!is.na(hour)) 

control_pm25_hourly <- pm25_community_hourly_seasonality %>% filter(cooking_period == "Control Hour") %>% 
  filter(!is.na(hour)) 




mean(cooktime_pm25_hourly$mean_pm25, na.rm = TRUE) 
sd(cooktime_pm25_hourly$mean_pm25, na.rm = TRUE) 

mean(not_cooktime_pm25_hourly$mean_pm25, na.rm = TRUE)  
sd(not_cooktime_pm25_hourly$mean_pm25, na.rm = TRUE) 

mean(control_pm25_hourly$mean_pm25, na.rm = TRUE)  
sd(control_pm25_hourly$mean_pm25, na.rm = TRUE)


# COOKTIME VS NOT COOKTIME VS CONTROL DURING HARMATTAN

mean(harmattan_pm25_hourly$mean_pm25[harmattan_pm25_hourly$cooking_period == "Active Cooking Hours"], na.rm = TRUE)
sd(harmattan_pm25_hourly$mean_pm25[harmattan_pm25_hourly$cooking_period == "Active Cooking Hours"], na.rm = TRUE)

mean(harmattan_pm25_hourly$mean_pm25[harmattan_pm25_hourly$cooking_period == "Non-Active Cooking Hours"], na.rm = TRUE)
sd(harmattan_pm25_hourly$mean_pm25[harmattan_pm25_hourly$cooking_period == "Non-Active Cooking Hours"], na.rm = TRUE)

mean(harmattan_pm25_hourly$mean_pm25[harmattan_pm25_hourly$cooking_period == "Control Hour"], na.rm = TRUE)
sd(harmattan_pm25_hourly$mean_pm25[harmattan_pm25_hourly$cooking_period == "Control Hour"], na.rm = TRUE)

# COOKTIME VS NOT COOKTIME NOT DURING HARMATTAN

mean(not_harmattan_pm25_hourly$mean_pm25[not_harmattan_pm25_hourly$cooking_period == "Active Cooking Hours"], na.rm = TRUE)
sd(not_harmattan_pm25_hourly$mean_pm25[not_harmattan_pm25_hourly$cooking_period == "Active Cooking Hours"], na.rm = TRUE)

mean(not_harmattan_pm25_hourly$mean_pm25[not_harmattan_pm25_hourly$cooking_period == "Non-Active Cooking Hours"], na.rm = TRUE)
sd(not_harmattan_pm25_hourly$mean_pm25[not_harmattan_pm25_hourly$cooking_period == "Non-Active Cooking Hours"], na.rm = TRUE)

mean(not_harmattan_pm25_hourly$mean_pm25[not_harmattan_pm25_hourly$cooking_period == "Control Hour"], na.rm = TRUE)
sd(not_harmattan_pm25_hourly$mean_pm25[not_harmattan_pm25_hourly$cooking_period == "Control Hour"], na.rm = TRUE)

```



### Spatial Correlation

```{r spatial correlation 25, echo=FALSE, message=FALSE, warning=FALSE}
generate_spatial_pollution_map(pm25_community, "pm25", missing_monitor_location)
```

### Calculate Moran I for Spatial Correlation

```{r moran i 25, echo=FALSE, message=FALSE, warning=FALSE}
calculate_moran_i(pm25_community, "pm25", missing_monitor_location)
```

**Interpretation of Moran I Statistic**

-   -1: The variable of interest is perfectly dispersed

-   0: The variable of interest is randomly dispersed

-   1: The variable of interest is perfectly clustered together

### Including proximity to road

```{r road 25, echo=FALSE, message=FALSE, warning=FALSE}
roads_filtered_grouped <- roads_filtered %>%
  mutate(highway = case_when(highway %in% c("secondary", "trunk", "tertiary") ~ "City Connection",
                             highway == "residential" ~ "Residential",
                             TRUE ~ "Other")) 

monitor_sf_road_dist <- prep_monitor_road_data(pm25_community, "pm25", missing_monitor_location, roads_filtered_grouped)

close_roads <- monitor_sf_road_dist %>% filter(distance_to_road < 100)

generate_spatial_pollution_road_map(monitor_sf_road_dist, roads_filtered_grouped, "pm25")

pm25_regression_results <- regress_polutant_road(close_roads, "pm25")

pm25_regression_results$plot_no_type

pm25_regression_results$plot_with_type

pm25_regression_results$summary_table
```


## PM 1 Colocation 

### Data Quality Check: Comparing PM 1 From Individual Monitors to Fleet Average

```{r colocation monitor fleet avg 1, fig.height=10, fig.width=7, echo=FALSE, message=FALSE, warning=FALSE}
# hourly comparison
compare_fleet_avg_monitor(pm1_colocation_hourly, "pm1", "hourly")

# Daily comparison
compare_fleet_avg_monitor(pm1_colocation_daily, "pm1", "daily")
```

#### PM 1 Hourly: Individual Monitor Regressed on Fleet Average
**Note: fleet average required at minimum 5 readings to be included in regression)**
```{r, echo=FALSE, message=FALSE, warning=FALSE}
pm1_hourly_regression_data <- pm1_colocation_hourly %>% 
  group_by(date, hour) %>% 
  summarize(offline_monitors = sum(is.na(mean_pm1))) %>%
  ungroup() %>%
  
  #add the PM data back
  left_join(pm1_colocation_hourly) %>%
  
  filter(offline_monitors < 35) %>%
  
  filter(!is.na(mean_pm1))


pm1_colocation_regression_results <- apply_regression(pm1_hourly_regression_data, "mean_pm1", "fleet_average_pm1")

datatable(pm1_colocation_regression_results)
```

#### PM 1 Daily: Individual Monitor Regressed on Fleet Average
**Note: fleet average required at minimum 5 readings to be included in regression)**
```{r, echo=FALSE, message=FALSE, warning=FALSE}
pm1_daily_regression_data <- pm1_colocation_daily %>% 
  group_by(date) %>% 
  summarize(offline_monitors = sum(is.na(mean_pm1))) %>%
  ungroup() %>%
  
  #add the PM data back
  left_join(pm1_colocation_daily) %>%
  
  filter(offline_monitors < 35) %>%
  
  filter(!is.na(mean_pm1))


pm1_colocation_regression_results <- apply_regression(pm1_daily_regression_data, "mean_pm1", "fleet_average_pm1")

datatable(pm1_colocation_regression_results)
```

### PM 1 Colocation MOD vs MOD-PM measures to check consistency

```{r, echo=FALSE, message=FALSE, warning=FALSE}
mod_pm_pm1 <- pm1_colocation_hourly %>%
  mutate(mod_pm = str_detect(monitor, "MOD-PM")) %>%
  group_by(mod_pm, date, hour) %>%
  summarize(mean_hourly_pm = mean(mean_pm1, na.rm = TRUE)) %>%
  ungroup() %>%
  filter(mod_pm == TRUE)

mod_pm1 <- pm1_colocation_hourly %>%
  mutate(mod_pm = str_detect(monitor, "MOD-PM")) %>%
  group_by(mod_pm, date, hour) %>%
  summarize(mean_hourly_pm = mean(mean_pm1, na.rm = TRUE)) %>%
  ungroup() %>%
  filter(mod_pm == FALSE)

# Extract the mean_hourly_pm vectors
MOD_PM_hourly_pm1_means <- mod_pm_pm1$mean_hourly_pm
MOD_hourly_pm1_means <- mod_pm1$mean_hourly_pm

# Perform t-test
t_test_result <- t.test(MOD_PM_hourly_pm1_means, MOD_hourly_pm1_means)

# Print the result
print(t_test_result)
```



### PM 1 Trend Heatmap

```{r colocation pm1 trends, echo=FALSE, message=FALSE, warning=FALSE}
generate_heatmap(pm1_colocation_hourly, "pm1")
```

## PM 1 Community Deployment

### Checking Individual Monitor to Fleet Average

```{r monitor vs fleet avg 1, fig.height=10, fig.width=7, echo=FALSE, message=FALSE, warning=FALSE}
# hourly comparison
compare_fleet_avg_monitor(pm1_community_hourly, "pm1", "hourly")

# Daily comparison
compare_fleet_avg_monitor(pm1_community_daily, "pm1", "daily")
```

#### PM 1 Hourly: Individual Monitor Regressed on Fleet Average
**Note: fleet average required at minimum 5 readings to be included in regression)**
```{r, echo=FALSE, message=FALSE, warning=FALSE}
pm1_hourly_regression_data <- pm1_community_hourly %>% 
  group_by(date, hour) %>% 
  summarize(offline_monitors = sum(is.na(mean_pm1))) %>%
  ungroup() %>%
  
  #add the PM data back
  left_join(pm1_community_hourly) %>%
  
  filter(offline_monitors < 35) %>%
  
  filter(!is.na(mean_pm1))


pm1_colocation_regression_results <- apply_regression(pm1_hourly_regression_data, "mean_pm1", "fleet_average_pm1")

datatable(pm1_colocation_regression_results)
```

#### PM 1 Daily: Individual Monitor Regressed on Fleet Average
**Note: fleet average required at minimum 5 readings to be included in regression)**
```{r, echo=FALSE, message=FALSE, warning=FALSE}
pm1_daily_regression_data <- pm1_community_daily %>% 
  group_by(date) %>% 
  summarize(offline_monitors = sum(is.na(mean_pm1))) %>%
  ungroup() %>%
  
  #add the PM data back
  left_join(pm1_community_daily) %>%
  
  filter(offline_monitors < 35) %>%
  
  filter(!is.na(mean_pm1))


pm1_colocation_regression_results <- apply_regression(pm1_daily_regression_data, "mean_pm1", "fleet_average_pm1")

datatable(pm1_colocation_regression_results)
```


### PM 1 Trends

```{r pm1 timelines, echo=FALSE, message=FALSE, warning=FALSE}
daily_pm1_all_devices <- pm1_community %>%
  group_by(date, monitor) %>%
  summarize(mean_pm1 = mean(pm1, na.rm = TRUE)) %>%
  ggplot(aes(x = date, y = mean_pm1, group = monitor)) +
  geom_line(color = "black", alpha = 0.2) +
  theme_minimal() +
  labs(y = "Mean PM 1",
       title = "Average PM 1 Accross All Devices")

daily_cooktime_pm1_all_devices <- pm1_community %>%
  filter(hour >= 15 & hour < 18) %>%
  group_by(monitor, date) %>%
  summarise(mean_pm1 = mean(pm1, na.rm = TRUE)) %>%
  ggplot(aes(x = date, y = mean_pm1, group = monitor)) +
  geom_line(color = "black", alpha = 0.4) +
  theme_minimal() +
  labs(y = "Mean PM 1 ",
       title = "Daily Average PM 1 for Each Monitor During Primary Cooking Hours (3-7)") 

grid.arrange(daily_pm1_all_devices, daily_cooktime_pm1_all_devices)

daily_pm1_timeline <- pm1_community %>%
  group_by(date) %>%
  summarize(mean_pm1 = mean(pm1, na.rm = TRUE)) %>%
  ggplot(aes(x = date, y = mean_pm1)) +
  geom_line() +
  theme_minimal() +
  labs(y = "Mean PM 1",
       title = "Average PM 1 Accross All Devices")

daily_cooktime_pm1_timeline <- pm1_community %>%
  filter(hour >= 15 & hour < 18) %>%
  group_by(date) %>%
  summarize(mean_pm1 = mean(pm1, na.rm = TRUE)) %>%
  ggplot(aes(x = date, y = mean_pm1)) +
  geom_line() +
  theme_minimal() +
  labs(y = "Mean PM 1",
       title = "Mean PM 1 Accross All Devices During Cooking Hours (3-7)")

grid.arrange(daily_pm1_timeline, daily_cooktime_pm1_timeline)
```

### PM 1 Heatmap

```{r pm1 trends, echo=FALSE, message=FALSE, warning=FALSE}
generate_heatmap(pm1_community_hourly, "pm1")
```


### Spatial Correlation

```{r spatial correlation 1, echo=FALSE, message=FALSE, warning=FALSE}
generate_spatial_pollution_map(pm1_community, "pm1", missing_monitor_location)
```

### Calculate Moran I for Spatial Correlation

```{r moran i 1, echo=FALSE, message=FALSE, warning=FALSE}
calculate_moran_i(pm1_community, "pm1", missing_monitor_location)
```

**Interpretation of Moran I Statistic**

-   -1: The variable of interest is perfectly dispersed

-   0: The variable of interest is randomly dispersed

-   1: The variable of interest is perfectly clustered together

### Including proximity to road

```{r road 1, echo=FALSE, message=FALSE, warning=FALSE}
monitor_sf_road_dist <- prep_monitor_road_data(pm1_community, "pm1", missing_monitor_location, roads)

generate_spatial_pollution_road_map(monitor_sf_road_dist, roads_filtered, "pm1")

pm1_regression_results <- regress_polutant_road(monitor_sf_road_dist, "pm1")

pm1_regression_results$plot

pm1_regression_results$summary_table
```





## PM 10 Colocation 

### Data Quality Check: Comparing PM 10 From Individual Monitors to Fleet Average

```{r colocation monitor fleet avg 10, fig.height=10, fig.width=7, echo=FALSE, message=FALSE, warning=FALSE}
# hourly comparison
compare_fleet_avg_monitor(pm10_colocation_hourly, "pm10", "hourly")

# Daily comparison
compare_fleet_avg_monitor(pm10_colocation_daily, "pm10", "daily")
```

#### PM 10 Hourly: Individual Monitor Regressed on Fleet Average
**Note: fleet average required at minimum 5 readings to be included in regression)**
```{r, echo=FALSE, message=FALSE, warning=FALSE}
pm10_hourly_regression_data <- pm10_colocation_hourly %>% 
  group_by(date, hour) %>% 
  summarize(offline_monitors = sum(is.na(mean_pm10))) %>%
  ungroup() %>%
  
  #add the PM data back
  left_join(pm10_colocation_hourly) %>%
  
  filter(offline_monitors < 35) %>%
  
  filter(!is.na(mean_pm10))


pm10_colocation_regression_results <- apply_regression(pm10_hourly_regression_data, "mean_pm10", "fleet_average_pm10")

datatable(pm10_colocation_regression_results)
```

#### PM 10 Daily: Individual Monitor Regressed on Fleet Average
**Note: fleet average required at minimum 5 readings to be included in regression)**
```{r, echo=FALSE, message=FALSE, warning=FALSE}
pm10_daily_regression_data <- pm10_colocation_daily %>% 
  group_by(date) %>% 
  summarize(offline_monitors = sum(is.na(mean_pm10))) %>%
  ungroup() %>%
  
  #add the PM data back
  left_join(pm10_colocation_daily) %>%
  
  filter(offline_monitors < 35) %>%
  
  filter(!is.na(mean_pm10))


pm10_colocation_regression_results <- apply_regression(pm10_daily_regression_data, "mean_pm10", "fleet_average_pm10")

datatable(pm10_colocation_regression_results)
```





### PM 10 Trends

```{r pm10 timeline, echo=FALSE, message=FALSE, warning=FALSE}
daily_pm10_all_devices <- pm10_community %>%
  group_by(date, monitor) %>%
  summarize(mean_pm10 = mean(pm10, na.rm = TRUE)) %>%
  ggplot(aes(x = date, y = mean_pm10, group = monitor)) +
  geom_line(color = "black", alpha = 0.2) +
  theme_minimal() +
  labs(y = "Mean PM 10",
       title = "Average PM 10 Accross All Devices")

daily_cooktime_pm10_all_devices <- pm10_community %>%
  filter(hour >= 15 & hour < 18) %>%
  group_by(monitor, date) %>%
  summarise(mean_pm10 = mean(pm10, na.rm = TRUE)) %>%
  ggplot(aes(x = date, y = mean_pm10, group = monitor)) +
  geom_line(color = "black", alpha = 0.4) +
  theme_minimal() +
  labs(y = "Mean PM 10 ",
       title = "Daily Average PM 10 for Each Monitor During Primary Cooking Hours (3-7)") 

grid.arrange(daily_pm10_all_devices, daily_cooktime_pm10_all_devices)

daily_pm10_timeline <- pm10_community %>%
  group_by(date) %>%
  summarize(mean_pm10 = mean(pm10, na.rm = TRUE)) %>%
  ggplot(aes(x = date, y = mean_pm10)) +
  geom_line() +
  theme_minimal() +
  labs(y = "Mean PM 10",
       title = "Average PM 10 Accross All Devices")

daily_cooktime_pm10_timeline <- pm10_community %>%
  filter(hour >= 15 & hour < 18) %>%
  group_by(date) %>%
  summarize(mean_pm10 = mean(pm10, na.rm = TRUE)) %>%
  ggplot(aes(x = date, y = mean_pm10)) +
  geom_line() +
  theme_minimal() +
  labs(y = "Mean PM 10",
       title = "Mean PM 10 Accross All Devices During Cooking Hours (3-7)")

grid.arrange(daily_pm10_timeline, daily_cooktime_pm10_timeline)
```







### Making maps for each size class

```{r}

# DURING HARMATTAN ----
pm25_summary <- pm25_corrected %>%
  filter(date >= harmattan_start & date < harmattan_end) %>%
  group_by(monitor) %>%
  summarise(mean_pm25 = mean(pm25, na.rm = TRUE),
            median_pm25 = median(pm25, na.rm = TRUE)) %>%
  right_join(monitor_points)


ggplot() +
  # Add Ghana regions within the bounding box
  geom_sf(data = regions, fill = ifelse(regions$region == "Bono East", "#f5e493", "#dadbe0"), color = "black", alpha = 0.5)  +
  
  # Add road data
  geom_sf(data = roads_filtered, color = "gray", size = 0.3) +  # Add roads data
  
  # Add monitor points with transparency within the bounding box
  geom_sf(data = pm25_summary, aes(geometry = geometry, color = mean_pm25), alpha = 0.7, size = 6) +

  # Zoom in to the bounding box
  coord_sf(xlim = c(-2.2, -1.3), ylim = c(7.6, 8.8), expand = FALSE) +
  
  # Customize
  labs(title = "Mean PM 2.5 for Each Monitor During Harmattan",
       x = "", 
       y = "") +
  theme_bw() +
  scale_color_gradient(low = "#0540a1", high = "#eb4c2d",
                       breaks = c(min(pm25_summary$mean_pm25), 75, 100, max(pm25_summary$mean_pm25)),
                       labels = c(paste(round(min(pm25_summary$mean_pm25),0)), "75", "100", paste(round(max(pm25_summary$mean_pm25),0))),
                       name = "PM 2.5 \n(µg/m³)") +
  theme(
    axis.text = element_text(size = 11), # Increase axis text size
    axis.ticks = element_line(size = 0.7), # Increase tick mark size
    plot.title = element_text(size = 14), # Increase plot title size
    legend.text = element_text(size = 12), # Increase legend text size
    legend.title = element_text(size = 12), # Increase legend title size
    legend.key.size = unit(1.3, "lines") # Increase legend tick mark size
  )






pm1_summary <- pm1_corrected %>%
  filter(date >= harmattan_start & date < harmattan_end) %>%
  group_by(monitor) %>%
  summarise(mean_pm1 = mean(pm1, na.rm = TRUE),
            median_pm1 = median(pm1, na.rm = TRUE)) %>%
  right_join(monitor_points)


ggplot() +
  # Add Ghana regions within the bounding box
  geom_sf(data = regions, fill = ifelse(regions$region == "Bono East", "#f5e493", "#dadbe0"), color = "black", alpha = 0.5)  +
  
  # Add road data
  geom_sf(data = roads_filtered, color = "gray", size = 0.3) +  # Add roads data
  
  # Add monitor points with transparency within the bounding box
  geom_sf(data = pm1_summary, aes(geometry = geometry, color = mean_pm1), alpha = 0.7, size = 6) +

  # Zoom in to the bounding box
  coord_sf(xlim = c(-2.2, -1.3), ylim = c(7.6, 8.8), expand = FALSE) +
  
  # Customize
  labs(title = "Mean PM 1 for Each Monitor During Harmattan",
       x = "", 
       y = "") +
  theme_bw() +
  scale_color_gradient(low = "#0540a1", high = "#eb4c2d", 
                       breaks = c(min(pm1_summary$mean_pm1), 60, 85, max(pm1_summary$mean_pm1)),
                       labels = c(paste(round(min(pm1_summary$mean_pm1),0)), "60", "85", paste(round(max(pm1_summary$mean_pm1),0))),
                       name = "PM 1 \n(µg/m³)") +
  theme(
    axis.text = element_text(size = 11), # Increase axis text size
    axis.ticks = element_line(size = 0.7), # Increase tick mark size
    plot.title = element_text(size = 14), # Increase plot title size
    legend.text = element_text(size = 12), # Increase legend text size
    legend.title = element_text(size = 12), # Increase legend title size
    legend.key.size = unit(1.3, "lines") # Increase legend tick mark size
  )








pm10_summary <- pm10_corrected %>%
  filter(date >= harmattan_start & date < harmattan_end) %>%
  group_by(monitor) %>%
  summarise(mean_pm10 = mean(pm10, na.rm = TRUE),
            median_pm10 = median(pm10, na.rm = TRUE)) %>%
  right_join(monitor_points)


ggplot() +
  # Add Ghana regions within the bounding box
  geom_sf(data = regions, fill = ifelse(regions$region == "Bono East", "#f5e493", "#dadbe0"), color = "black", alpha = 0.5)  +
  
  # Add road data
  geom_sf(data = roads_filtered, color = "gray", size = 0.3) +  # Add roads data
  
  # Add monitor points with transparency within the bounding box
  geom_sf(data = pm10_summary, aes(geometry = geometry, color = mean_pm10), alpha = 0.7, size = 6) +

  # Zoom in to the bounding box
  coord_sf(xlim = c(-2.2, -1.3), ylim = c(7.6, 8.8), expand = FALSE) +
  
  # Customize
  labs(title = "Mean PM 10 for Each Monitor During Harmattan",
       x = "", 
       y = "") +
  theme_bw() +
  scale_color_gradient(low = "#0540a1", high = "#eb4c2d",
                       breaks = c(min(pm10_summary$mean_pm10, na.rm = TRUE), 210, 365, max(pm10_summary$mean_pm10)),
                       labels = c(paste(round(min(pm10_summary$mean_pm10),0)), "210", "365", paste(round(max(pm10_summary$mean_pm10),0))),
                       name = "PM 10 \n(µg/m³)") +
  theme(
    axis.text = element_text(size = 11), # Increase axis text size
    axis.ticks = element_line(size = 0.7), # Increase tick mark size
    plot.title = element_text(size = 14), # Increase plot title size
    legend.text = element_text(size = 12), # Increase legend text size
    legend.title = element_text(size = 12), # Increase legend title size
    legend.key.size = unit(1.3, "lines") # Increase legend tick mark size
  )


# NOT DURING HARMATTTAN ----

pm25_summary <- pm25_corrected %>%
  filter(date < harmattan_start | date >= harmattan_end) %>%
  group_by(monitor) %>%
  summarise(mean_pm25 = mean(pm25, na.rm = TRUE),
            median_pm25 = median(pm25, na.rm = TRUE)) %>%
  right_join(monitor_points)


ggplot() +
  # Add Ghana regions within the bounding box
  geom_sf(data = regions, fill = ifelse(regions$region == "Bono East", "#f5e493", "#dadbe0"), color = "black", alpha = 0.5)  +
  
  # Add road data
  geom_sf(data = roads_filtered, color = "gray", size = 0.3) +  # Add roads data
  
  # Add monitor points with transparency within the bounding box
  geom_sf(data = pm25_summary, aes(geometry = geometry, color = mean_pm25), alpha = 0.7, size = 6) +

  # Zoom in to the bounding box
  coord_sf(xlim = c(-2.2, -1.3), ylim = c(7.6, 8.8), expand = FALSE) +
  
  # Customize
  labs(title = "Mean PM 2.5 for Each Monitor Outside of Harmattan",
       x = "", 
       y = "") +
  theme_bw() +
  scale_color_gradient(low = "#0540a1",  high = "#eb4c2d",
                       breaks = c(min(pm25_summary$mean_pm25, na.rm = TRUE), 45,70, max(pm25_summary$mean_pm25)),
                       labels = c(paste(round(min(pm25_summary$mean_pm25),0)), "45", "70", paste(round(max(pm25_summary$mean_pm25),0))),
                       name = "PM 2.5 \n(µg/m³)") +
  theme(
    axis.text = element_text(size = 11), # Increase axis text size
    axis.ticks = element_line(size = 0.7), # Increase tick mark size
    plot.title = element_text(size = 14), # Increase plot title size
    legend.text = element_text(size = 12), # Increase legend text size
    legend.title = element_text(size = 12), # Increase legend title size
    legend.key.size = unit(1.3, "lines") # Increase legend tick mark size
  )



## DECISION TO RECREATE PLOT ABOVE WITH 3 POINT COLOR SCALE SO OUTLIER DOESN'T SKEW VARIABILITY OF OTHER POINTS

ggplot() +
  # Add Ghana regions within the bounding box
  geom_sf(data = regions, fill = ifelse(regions$region == "Bono East", "#f5e493", "#dadbe0"), color = "black", alpha = 0.5)  +
  
  # Add road data
  geom_sf(data = roads_filtered, color = "gray", size = 0.3) +  # Add roads data
  
  # Add monitor points with transparency within the bounding box
  geom_sf(data = pm25_summary, aes(geometry = geometry, color = mean_pm25), alpha = 0.7, size = 6) +

  # Zoom in to the bounding box
  coord_sf(xlim = c(-2.2, -1.3), ylim = c(7.6, 8.8), expand = FALSE) +
  
  # Customize
  labs(title = "Mean PM 2.5 for Each Monitor Outside of Harmattan",
       x = "", 
       y = "") +
  theme_bw() +
  scale_color_gradient2(low = "#0540a1", mid = "#eb4c2d", high = "goldenrod", 
                        midpoint = 55,  # Set the midpoint value
                        breaks = c(min(pm25_summary$mean_pm25, na.rm = TRUE), 55, max(pm25_summary$mean_pm25)),
                        labels = c(paste(round(min(pm25_summary$mean_pm25),0)), "55", paste(round(max(pm25_summary$mean_pm25),0))),
                        name = "PM 2.5 \n(µg/m³)") +
  theme(
    axis.text = element_text(size = 11), # Increase axis text size
    axis.ticks = element_line(size = 0.7), # Increase tick mark size
    plot.title = element_text(size = 14), # Increase plot title size
    legend.text = element_text(size = 12), # Increase legend text size
    legend.title = element_text(size = 12), # Increase legend title size
    legend.key.size = unit(1.3, "lines") # Increase legend tick mark size
  )







pm1_summary <- pm1_corrected %>%
  filter(date < harmattan_start | date <= harmattan_end) %>%
  group_by(monitor) %>%
  summarise(mean_pm1 = mean(pm1, na.rm = TRUE),
            median_pm1 = median(pm1, na.rm = TRUE)) %>%
  right_join(monitor_points)


ggplot() +
  # Add Ghana regions within the bounding box
  geom_sf(data = regions, fill = ifelse(regions$region == "Bono East", "#f5e493", "#dadbe0"), color = "black", alpha = 0.5)  +
  
  # Add road data
  geom_sf(data = roads_filtered, color = "gray", size = 0.3) +  # Add roads data
  
  # Add monitor points with transparency within the bounding box
  geom_sf(data = pm1_summary, aes(geometry = geometry, color = mean_pm1), alpha = 0.7, size = 6) +

  # Zoom in to the bounding box
  coord_sf(xlim = c(-2.2, -1.3), ylim = c(7.6, 8.8), expand = FALSE) +
  
  # Customize
  labs(title = "Mean PM 1 for Each Monitor Outside of Harmattan",
       x = "", 
       y = "") +
  theme_bw() +
  scale_color_gradient(low = "#0540a1", high = "#eb4c2d", 
                       breaks = c(min(pm1_summary$mean_pm1), 40, 55, max(pm1_summary$mean_pm1)),
                       labels = c(paste(round(min(pm1_summary$mean_pm1),0)), "40", "55", paste(round(max(pm1_summary$mean_pm1),0))),
                       name = "PM 1 \n(µg/m³)") +
  theme(
    axis.text = element_text(size = 11), # Increase axis text size
    axis.ticks = element_line(size = 0.7), # Increase tick mark size
    plot.title = element_text(size = 14), # Increase plot title size
    legend.text = element_text(size = 12), # Increase legend text size
    legend.title = element_text(size = 12), # Increase legend title size
    legend.key.size = unit(1.3, "lines") # Increase legend tick mark size
  )








pm10_summary <- pm10_corrected %>%
  filter(date < harmattan_start | date <= harmattan_end) %>%
  group_by(monitor) %>%
  summarise(mean_pm10 = mean(pm10, na.rm = TRUE),
            median_pm10 = median(pm10, na.rm = TRUE)) %>%
  right_join(monitor_points)


ggplot() +
  # Add Ghana regions within the bounding box
  geom_sf(data = regions, fill = ifelse(regions$region == "Bono East", "#f5e493", "#dadbe0"), color = "black", alpha = 0.5)  +
  
  # Add road data
  geom_sf(data = roads_filtered, color = "gray", size = 0.3) +  # Add roads data
  
  # Add monitor points with transparency within the bounding box
  geom_sf(data = pm10_summary, aes(geometry = geometry, color = mean_pm10), alpha = 0.7, size = 6) +

  # Zoom in to the bounding box
  coord_sf(xlim = c(-2.2, -1.3), ylim = c(7.6, 8.8), expand = FALSE) +
  
  # Customize
  labs(title = "Mean PM 10 for Each Monitor Outside of Harmattan",
       x = "", 
       y = "") +
  theme_bw() +
  scale_color_gradient(low = "#0540a1", high = "#eb4c2d", 
                       breaks = c(min(pm10_summary$mean_pm10, na.rm = TRUE), 140, 230, max(pm10_summary$mean_pm10)),
                       labels = c(paste(round(min(pm10_summary$mean_pm10),0)), "140", "230", paste(round(max(pm10_summary$mean_pm10),0))),
                       name = "PM 10 \n(µg/m³)") +
  theme(
    axis.text = element_text(size = 11), # Increase axis text size
    axis.ticks = element_line(size = 0.7), # Increase tick mark size
    plot.title = element_text(size = 14), # Increase plot title size
    legend.text = element_text(size = 12), # Increase legend text size
    legend.title = element_text(size = 12), # Increase legend title size
    legend.key.size = unit(1.3, "lines") # Increase legend tick mark size
  )

```




## HEATMAP CUSTOMIZATION

```{r}
## PM 25
 
pm25_community_hourly <- pm25_community_hourly %>%
    mutate(date = ymd(date))  # Use lubridate to ensure date conversion
  
heat_map_data <- pm25_community_hourly %>%
    group_by(date, hour) %>%
    summarize(mean_pm25 = mean(mean_pm25, na.rm = TRUE), .groups = 'drop') %>%
    ungroup() %>%
    mutate(day = day(date),
           month = factor(format(date, "%b"), levels = c("Aug", "Sep", "Oct", "Nov", "Dec", "Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul")),
           year = year(date)) %>%
  mutate(log_mean_pm25 = log(mean_pm25))

ggplot(heat_map_data, aes(day, hour, fill = mean_pm25)) +
    geom_tile(color = NA, size = 0) + 
    scale_fill_viridis(name = "PM 2.5 \n(µg/m³)", option = "F") + 
    facet_grid(~month) +
    scale_y_continuous(trans = "reverse", breaks = unique(heat_map_data$hour)) +
    scale_x_continuous(breaks = c(1, 10, 20, 31)) +
    theme_minimal(base_size = 8) +
    labs(x = "Day",
         y = "Hour") + 
    theme(legend.position = "right",
          plot.title = element_text(size = 14, hjust = 0),
          axis.text.y = element_text(size = 10),
          strip.background = element_rect(colour = "white"),
          strip.text = element_text(size = 13),   # Increase facet group text size
          axis.ticks = element_blank(),
          axis.text = element_text(size = 10),
          legend.title = element_text(size = 12),
          legend.text = element_text(size = 11),
          axis.title.y = element_text(size = 14),
          axis.title.x = element_text(size = 14),
          
          panel.background = element_blank())




ggplot(heat_map_data, aes(day, hour, fill = mean_pm25)) +
    geom_tile(color = NA, size = 0) + 
    scale_fill_gradientn(
        colors = c("#221f26", "#3f2363", "#872CA2", "#D44292", "#F98477", "#EDD9A1", "#fffceb"), # Specify your custom colors
        values = scales::rescale(c(0, 5, 12, 20, 40, 60, 100)), # Set breaks; adjust these values based on your data range
        name = "PM 2.5 \n(µg/m³)"
    ) + 
    facet_grid(~month) +
    scale_y_continuous(trans = "reverse", breaks = unique(heat_map_data$hour)) +
    scale_x_continuous(breaks = c(1, 10, 20, 31)) +
    theme_minimal(base_size = 8) +
    labs(x = "Day",
         y = "Hour") + 
    theme(legend.position = "right",
          plot.title = element_text(size = 14, hjust = 0),
          axis.text.y = element_text(size = 10),
          strip.background = element_rect(colour = "white"),
          strip.text = element_text(size = 13),   # Increase facet group text size
          axis.ticks = element_blank(),
          axis.text = element_text(size = 10),
          legend.title = element_text(size = 12),
          legend.text = element_text(size = 11),
          axis.title.y = element_text(size = 14),
          axis.title.x = element_text(size = 14),
          panel.background = element_blank())







## PM 10 ----

pm10_community_hourly <- pm10_community_hourly %>%
    mutate(date = ymd(date))  # Use lubridate to ensure date conversion
  
heat_map_data <- pm10_community_hourly %>%
    group_by(date, hour) %>%
    summarize(mean_pm10 = mean(mean_pm10, na.rm = TRUE), .groups = 'drop') %>%
    ungroup() %>%
    mutate(day = day(date),
           month = factor(format(date, "%b"), levels = c("Aug", "Sep", "Oct", "Nov", "Dec", "Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul")),
           year = year(date)) %>%
  mutate(log_mean_pm10 = log(mean_pm10))

ggplot(heat_map_data, aes(day, hour, fill = mean_pm10)) +
    geom_tile(color = NA, size = 0) + 
    scale_fill_gradientn(
        colors = c("#221f26", "#3f2363", "#872CA2", "#D44292", "#F98477", "#EDD9A1", "#fffceb"), # Specify your custom colors
        values = scales::rescale(c(0, 5, 10, 20, 40, 60, 100)), # Set breaks; adjust these values based on your data range
        name = "PM 10 \n(µg/m³)"
    ) + 
    facet_grid(~month) +
    scale_y_continuous(trans = "reverse", breaks = unique(heat_map_data$hour)) +
    scale_x_continuous(breaks = c(1, 10, 20, 31)) +
    theme_minimal(base_size = 8) +
    labs(x = "Day",
         y = "Hour") + 
    theme(legend.position = "right",
          plot.title = element_text(size = 14, hjust = 0),
          axis.text.y = element_text(size = 10),
          strip.background = element_rect(colour = "white"),
          strip.text = element_text(size = 13),   # Increase facet group text size
          axis.ticks = element_blank(),
          axis.text = element_text(size = 10),
          legend.title = element_text(size = 12),
          legend.text = element_text(size = 11),
          axis.title.y = element_text(size = 14),
          axis.title.x = element_text(size = 14),
          
          panel.background = element_blank())



ggplot(heat_map_data, aes(day, hour, fill = mean_pm10)) +
    geom_tile(color = NA, size = 0) + 
    scale_fill_gradientn(
        colors = c("#221f26", "#3f2363", "#872CA2", "#D44292", "#F98477", "#EDD9A1", "#fffceb"), 
        values = scales::rescale(c(0, 5, 10, 20, 40, 60, 100)),
        breaks = c(5, 250, 500, 750, 950),  # Add more breaks
        labels = c("5", "250", "500", "750", "950"),  # Corresponding labels
        name = "PM 10 \n(µg/m³)"
    ) + 
    facet_grid(~month) +
    scale_y_continuous(trans = "reverse", breaks = unique(heat_map_data$hour)) +
    scale_x_continuous(breaks = c(1, 10, 20, 31)) +
    theme_minimal(base_size = 8) +
    labs(x = "Day",
         y = "Hour") + 
    theme(legend.position = "right",
          plot.title = element_text(size = 14, hjust = 0),
          axis.text.y = element_text(size = 10),
          strip.background = element_rect(colour = "white"),
          strip.text = element_text(size = 13),  
          axis.ticks = element_blank(),
          axis.text = element_text(size = 10),
          legend.title = element_text(size = 12),
          legend.text = element_text(size = 11),
          axis.title.y = element_text(size = 14),
          axis.title.x = element_text(size = 14),
          panel.background = element_blank())


```




## Moran I look nicer

```{r}

# Define parameters
pollutant <- "pm25"
k <- 5

#file_name <- "pm25_moran_table.png"


## DURING HARMATTAN ----

# Calculate summary statistics
summary_stats <- pm25_corrected %>%
  filter(date >= harmattan_start & date < harmattan_end) %>%
  group_by(monitor) %>%
  summarize(
    mean_value = mean(!!sym(pollutant), na.rm = TRUE),
    median_value = median(!!sym(pollutant), na.rm = TRUE),
    sd_value = sd(!!sym(pollutant), na.rm = TRUE)
  )

monitor_data <- monitor_points %>%
  left_join(summary_stats, by = "monitor")

# Convert monitor data to spatial object using sf
monitor_sf <- st_as_sf(monitor_data)

# Calculate distance-based spatial weights
coords <- st_coordinates(monitor_sf)
nb <- knn2nb(knearneigh(coords, k = k)) # k-nearest neighbors
listw <- nb2listw(nb, style = "W")

# Calculate Moran's I
moran_result <- moran.test(monitor_sf$mean_value, listw)

# Extract relevant values directly
observed_value <- round(as.numeric(moran_result$estimate["Moran I statistic"]), 4)
expected_value <- round(as.numeric(moran_result$estimate["Expectation"]), 4)
variance <- round(as.numeric(moran_result$estimate["Variance"]), 4)
z_score <- round(as.numeric(moran_result$statistic), 4)
p_value <- round(as.numeric(moran_result$p.value), 4)

# Create the Moran's I results table
moran_table <- data.frame(
  Test = "Moran's I Statistic",
  `Observed Value` = observed_value,
  `Expected Value` = expected_value,
  `Variance` = variance,
  `Z-score` = z_score,
  `p-value` = p_value
)

# Format the table using kableExtra
formatted_table <- moran_table %>%
  kable("html", booktabs = TRUE, caption = "Moran's I Test Results: PM 2.5 Shows Spatial Correlation During Harmattan") %>%
  kable_styling(full_width = FALSE, position = "center", latex_options = c("hold_position", "striped"))



## OUTSIDE OF HARMATTAN ----

# Calculate summary statistics
summary_stats <- pm25_corrected %>%
  filter(date < harmattan_start | date <= harmattan_end) %>%
  group_by(monitor) %>%
  summarize(
    mean_value = mean(!!sym(pollutant), na.rm = TRUE),
    median_value = median(!!sym(pollutant), na.rm = TRUE),
    sd_value = sd(!!sym(pollutant), na.rm = TRUE)
  )

monitor_data <- monitor_points %>%
  left_join(summary_stats, by = "monitor")

# Convert monitor data to spatial object using sf
monitor_sf <- st_as_sf(monitor_data)

# Calculate distance-based spatial weights
coords <- st_coordinates(monitor_sf)
nb <- knn2nb(knearneigh(coords, k = k)) # k-nearest neighbors
listw <- nb2listw(nb, style = "W")

# Calculate Moran's I
moran_result <- moran.test(monitor_sf$mean_value, listw)

# Extract relevant values directly
observed_value <- round(as.numeric(moran_result$estimate["Moran I statistic"]), 4)
expected_value <- round(as.numeric(moran_result$estimate["Expectation"]), 4)
variance <- round(as.numeric(moran_result$estimate["Variance"]), 4)
z_score <- round(as.numeric(moran_result$statistic), 4)
p_value <- round(as.numeric(moran_result$p.value), 4)

# Create the Moran's I results table
moran_table <- data.frame(
  Test = "Moran's I Statistic",
  `Observed Value` = observed_value,
  `Expected Value` = expected_value,
  `Variance` = variance,
  `Z-score` = z_score,
  `p-value` = p_value
)

# Format the table using kableExtra
formatted_table <- moran_table %>%
  kable("html", booktabs = TRUE, caption = "Moran's I Test Results: PM 2.5 Shows Spatial Correlation Outside of Harmattan") %>%
  kable_styling(full_width = FALSE, position = "center", latex_options = c("hold_position", "striped"))




```





```{r}
monitor_names <- tibble::tribble(
  ~monitor, ~description, ~location,
  "MOD-PM-00887", "Babator", "Babatokuma GH",
  "MOD-PM-00881", "Mo-line", "Kintampo GH",
  "MOD-PM-01052", "Kokuma", "Kokuma GH",
  "MOD-PM-01055", "Krabonso", "Krabonso GH",
  "MOD-PM-00893", "Jato Akura", "Jato Akura GH",
  "MOD-PM-01060", "Kintampo Central College Areas", "Kintampo GH",
  "MOD-PM-00899", "Nante", "Nante GH",
  "MOD-00399", "Dwenewoho", "Dwenewoho GH",
  "MOD-PM-01057", "Sorunuase", "Sorunuase GH",
  "MOD-PM-00877", "Pamdu", "Pamdu GH",
  "MOD-PM-00898", "Anokyekrom", "Anokyekrom GH",
  "MOD-PM-00900", "Kawampe", "Kawampe GH",
  "MOD-PM-00883", "Portor", "Portor GH",
  "MOD-00398", "Kurawura Akura", "Kurawura Akura GH",
  "MOD-PM-00884", "KHRC Residency", "Kintampo GH",
  "MOD-PM-01056", "Gulumpe", "Gulumpe GH",
  "MOD-PM-00879", "Dawadawa", "Dawadawa GH",
  "MOD-PM-00897", "Pramposo", "Pramposo GH",
  "MOD-PM-01053", "Anyima", "Anyima GH",
  "MOD-PM-00880", "Kadelso", "Kadelso GH",
  "MOD-PM-01051", "Weila", "Weila GH",
  "MOD-PM-01058", "Krutakyi", "Krutakyi GH",
  "MOD-PM-00895", "Ntankro", "Kintampo GH",
  "MOD-PM-00889", "Beposo", "Beposo GH",
  "MOD-PM-00882", "Apaaso", "Kintampo GH",
  "MOD-PM-01059", "Paninamisa", "Paninamisa GH",
  "MOD-00397", "Apesika", "Apesika GH",
  "MOD-PM-00878", "Jema Pentecost", "Jema GH",
  "MOD-00401", "New Longoro", "New Longoro GH",
  "MOD-PM-00891", "Habitat", "Kintampo GH",
  "MOD-PM-00885", "Kwabia", "Kwabia GH",
  "MOD-PM-00890", "Akora", "Akora GH",
  "MOD-PM-00876", "Amoma", "Amoma GH",
  "MOD-PM-00888", "Nante Zongo", "Nante Zongo GH",
  "MOD-PM-00894", "Alhassan Akura", "Alhassan Akura GH",
  "MOD-00400", "Kintampo-PTC Area (Magazine)", "Kintampo GH",
  "MOD-PM-01054", "Ampoma", "Ampoma GH",
  "MOD-PM-00896", "Asantekwaa", "Asantekwaa GH",
  "MOD-PM-00886", "Jema Zongo", "Jema GH",
  "MOD-PM-00892", "Atta Akura", "Atta Akura GH"
)

monitor_names <- monitor_names %>%
  mutate(location = gsub(" GH", "", location)) %>%
  mutate(locations = if_else(grepl("Kintampo", location),
                            paste(description, location, sep = ", "),
                            location))




# Custom function to add line breaks at a specific length
add_line_breaks <- function(x, width) {
  sapply(x, function(y) {
    if (nchar(y) > width) {
      paste(strwrap(y, width = width), collapse = "\n")
    } else {
      y
    }
  })
}

# Apply custom function to locations
monitor_names <- monitor_names %>%
  mutate(locations = if_else(grepl("Kintampo", location),
                             paste(description, "Kintampo", sep = "\n"),
                             location)) %>%
  mutate(locations = add_line_breaks(locations, width = 20))

# Plot with updated facet labels
pm25_community %>%
  left_join(monitor_names) %>%
  group_by(date, locations) %>%
  summarize(mean_pm25 = mean(pm25, na.rm = TRUE)) %>%
  ggplot(aes(x = date, y = mean_pm25)) +
  geom_line(color = "black", alpha = 1) +
  theme_minimal() +
  facet_wrap(~locations) +
  labs(y = "Mean PM 2.5") +
  theme(
    axis.text = element_text(size = 11), # Increase axis text size
    axis.title = element_text(size = 14),
    strip.text = element_text(size = 11)  # Increase facet group text size
  )

```







```{r}
roads_filtered_grouped <- roads_filtered %>%
  mutate(highway = case_when(highway %in% c("secondary", "trunk", "tertiary") ~ "City Connection",
                             highway == "residential" ~ "Residential",
                             TRUE ~ "Other")) 



missing_monitor_location_sf <- missing_monitor_location %>% select(everything(), geometry) %>% st_as_sf()

monitor_buffers <- st_buffer(missing_monitor_location_sf, dist = 100)

roads_within_buffers <- st_intersection(roads_filtered_grouped, monitor_buffers) %>%
  mutate(road_length = st_length(geometry))

total_road_length_per_monitor <- roads_within_buffers %>%
  group_by(monitor) %>%
  summarize(total_length = sum(road_length)) %>%
  ungroup() %>%
  st_drop_geometry()

monitor_with_road_lengths <- left_join(missing_monitor_location_sf, total_road_length_per_monitor, by = "monitor") %>%
  mutate(total_length = as.numeric(total_length)) %>%
  mutate(total_length = case_when(
    is.na(total_length) ~ 0,
    TRUE ~ total_length
  )) 
  

summary_stats_harmattan <- pm25_community %>%
  filter(date >= harmattan_start |
         date < harmattan_end) %>%
    group_by(monitor) %>%
    summarize(
      mean_value = mean(pm25, na.rm = TRUE),
      median_value = median(pm25, na.rm = TRUE),
      sd_value = sd(pm25, na.rm = TRUE)
    )

# Merge with monitor location data
monitor_data <- monitor_with_road_lengths %>%
  left_join(summary_stats_harmattan, by = "monitor")
  
  # Convert monitor data to spatial object using sf
monitor_sf <- st_as_sf(monitor_data)
  
  # Calculate distance to nearest road and get the nearest road index
nearest_roads <- st_nearest_feature(monitor_sf, roads_filtered_grouped)
distances <- st_distance(monitor_sf, roads_filtered_grouped[nearest_roads, ], by_element = TRUE)
  
  # Add distances and nearest road type to the monitor data
monitor_sf <- monitor_sf %>%
  mutate(
    distance_to_road = as.numeric(distances),
    road_type = roads_filtered_grouped$highway[nearest_roads],
    road_surface = roads_filtered_grouped$surface[nearest_roads]

    ) 

close_roads <- monitor_sf %>% 
  #filter(distance_to_road < 200) %>%
  left_join(monitor_community_populations, by = "monitor") %>%
  mutate(main_road = ifelse(location %in% c("Amoma", "Anyima", "Krabonso", "Ampoma", "Kokuma"), 
                            "Not Main Road", 
                            "Main Road"))





population_road_model <- lm(log(mean_value) ~ log(population) + main_road, data = close_roads)

summary(population_road_model)

# Model summaries
model_name_pop_road <- paste("Log of Mean PM 2.5 Regressed on Population and Primary Road Status")

model_list <- setNames(list(population_road_model), 
                       c(model_name_pop_road))

summary_table <- modelsummary(
  model_list, 
  #coef_rename = c("distance_to_road" = "Distance to Road", "total_length" = "Road Length within 100m Buffer"),
  gof_omit = "AIC|BIC|Log.Lik",
  estimate = "{estimate} ({std.error}){stars}",
  statistic = "p.value"
)




coefs <- summary(population_road_model)$coefficients
slope <- coefs["log(population)", "Estimate"]
p_value <- coefs["log(population)", "Pr(>|t|)"]
  

# Plot considering road type
close_roads %>%
  #filter(population < 20000) %>%
  ggplot(aes(x = log(population), y = log(mean_value))) +
  geom_point(aes(x = log(population), y = log(mean_value))) +
  geom_smooth(aes(x = log(population), y = log(mean_value)), method = "lm", se = FALSE) +
  annotate("text", x = Inf, y = Inf, label = paste("Slope:", round(slope, 2), "\nP-value:", round(p_value, 3)),
             hjust = 1.7, vjust = 7, size = 4, color = "black") +
  labs(x = "Log of Community Population",
       y = "Log of Average PM 2.5 Reading") +
  theme_minimal() +
  theme(
    axis.text = element_text(size = 11), # Increase axis text size
    axis.ticks = element_line(size = 0.7), # Increase tick mark size
    legend.text = element_text(size = 10), # Increase legend text size
    legend.title = element_text(size = 12), # Increase legend title size
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
  )
```


## roads excluding harmattan
```{r}

summary_stats_non_harmattan <- pm25_community %>%
  filter(date < harmattan_start |
         date >= harmattan_end) %>%
    group_by(monitor) %>%
    summarize(
      mean_value = mean(pm25, na.rm = TRUE),
      median_value = median(pm25, na.rm = TRUE),
      sd_value = sd(pm25, na.rm = TRUE)
    )

# Merge with monitor location data
monitor_data <- monitor_with_road_lengths %>%
  left_join(summary_stats_non_harmattan, by = "monitor")
  
  # Convert monitor data to spatial object using sf
monitor_sf <- st_as_sf(monitor_data)
  
  # Calculate distance to nearest road and get the nearest road index
nearest_roads <- st_nearest_feature(monitor_sf, roads_filtered_grouped)
distances <- st_distance(monitor_sf, roads_filtered_grouped[nearest_roads, ], by_element = TRUE)
  
  # Add distances and nearest road type to the monitor data
monitor_sf <- monitor_sf %>%
  mutate(
    distance_to_road = as.numeric(distances),
    road_type = roads_filtered_grouped$highway[nearest_roads]
    ) 

close_roads <- monitor_sf %>% 
  #filter(distance_to_road < 200) %>%
  left_join(monitor_community_populations, by = "monitor") %>%
  mutate(main_road = ifelse(location %in% c("Amoma", "Anyima", "Krabonso", "Ampoma", "Kokuma"), 
                            "Not Main Road", 
                            "Main Road"))

population_road_model <-lm(log(mean_value) ~ log(population) + main_road, data = close_roads)

# Model summaries
model_name_pop_road <- paste("Log of Mean PM 2.5 Regressed on Population and Primary Road Status")

model_list <- setNames(list(population_road_model), 
                       c(model_name_pop_road))

summary_table <- modelsummary(
  model_list, 
  #coef_rename = c("distance_to_road" = "Distance to Road", "total_length" = "Road Length within 100m Buffer"),
  gof_omit = "AIC|BIC|Log.Lik",
  estimate = "{estimate} ({std.error}){stars}",
  statistic = "p.value"
)
  
  
  
  
  

coefs <- summary(population_road_model)$coefficients
slope <- coefs["log(population)", "Estimate"]
p_value <- coefs["log(population)", "Pr(>|t|)"]
  

# Plot considering road type
close_roads %>%
  #filter(population < 20000) %>%
  ggplot(aes(x = log(population), y = log(mean_value))) +
  geom_point(aes(x = log(population), y = log(mean_value))) +
  geom_smooth(aes(x = log(population), y = log(mean_value)), method = "lm", se = FALSE) +
  annotate("text", x = Inf, y = Inf, label = paste("Slope:", round(slope, 2), "\nP-value:", round(p_value, 3)),
             hjust = 1.7, vjust = 7, size = 4, color = "black") +
  labs(x = "Log of Community Population",
       y = "Log of Average PM 2.5 Reading") +
  theme_minimal() +
  theme(
    axis.text = element_text(size = 11), # Increase axis text size
    axis.ticks = element_line(size = 0.7), # Increase tick mark size
    legend.text = element_text(size = 10), # Increase legend text size
    legend.title = element_text(size = 12), # Increase legend title size
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
  )



  
  



```



## DATA REQUEST FROM STEVE

```{r}
# average_each_monitor <- full_join(pm1_community_hourly, pm25_community_hourly) %>%
#   mutate(date = as.Date(date)) %>%
#   full_join(pm10_community_hourly) %>%
#   filter(!is.na(hour)) %>%
#   select(monitor, date, hour, mean_pm1, mean_pm25, mean_pm10) %>%
#   mutate(harmattan = ifelse(date >= harmattan_start & date <= harmattan_end, "Harmattan", "Not Harmattan"))
# 
# 
# average_across_monitors <- average_each_monitor %>%
#   group_by(date, hour) %>% 
#   summarise(
#     avg_pm1 = mean(mean_pm1, na.rm = TRUE),
#     avg_pm25 = mean(mean_pm25, na.rm = TRUE),
#     avg_pm10 = mean(mean_pm10, na.rm = TRUE),
#     n_pm1 = sum(!is.na(mean_pm1)),
#     n_pm25 = sum(!is.na(mean_pm25)),
#     n_pm10 = sum(!is.na(mean_pm10))
#   ) %>%
#     mutate(harmattan = ifelse(date >= harmattan_start & date <= harmattan_end, "Harmattan", "Not Harmattan"))
# 
# 
# write_csv(average_across_monitors, here("fleet_diurnal_hourly_average.csv"))
# 



```


## DATA REQUEST FROM KHOLISWA

```{r}
# write_csv(pm1_community_merged, here("data", "pm1_community_merged.csv"))
# write_csv(pm25_community_merged, here("data", "pm25_community_merged.csv"))
# write_csv(pm10_community_merged, here("data", "pm10_community_merged.csv"))
# 
# pm1_community_merged <- fread(here("data", "pm1_community_merged.csv"), showProgress = TRUE) %>%
#   filter(!is.na(hour))
# 
# pm25_community_merged <- fread(here("data", "pm25_community_merged.csv"), showProgress = TRUE) %>%
#   filter(!is.na(hour))
# 
# pm10_community_merged <- fread(here("data", "pm10_community_merged.csv"), showProgress = TRUE)%>%
#   filter(!is.na(hour))
# 
# 
# first_join <- full_join(pm1_community_merged, pm25_community_merged)
# 
# second_join <- full_join(first_join, pm10_community_merged)
# 
# 
# 
# write_csv(pm1_corrected, here("data", "pm1_corrected.csv"))
# write_csv(pm25_corrected, here("data", "pm25_corrected.csv"))
# write_csv(pm10_corrected, here("data", "pm10_corrected.csv"))
# 
# 
# pm1_corrected <- fread(here("data", "pm1_corrected.csv"), showProgress = TRUE) %>%
#   filter(!is.na(hour))
# 
# pm25_corrected <- fread(here("data", "pm25_corrected.csv"), showProgress = TRUE) %>%
#   filter(!is.na(hour))
# 
# pm10_corrected <- fread(here("data", "pm10_corrected.csv"), showProgress = TRUE)%>%
#   filter(!is.na(hour))
# 
# 
# 
# first_join <- full_join(pm1_corrected, pm25_corrected)
# 
# second_join <- full_join(first_join, pm10_corrected)
# 
# 
# 
# write_csv(second_join, here("data", "corrected_full_data.csv"))
# 

```


```{r, include = FALSE}
# Load necessary libraries
library(stringr)
library(fs)

# Define base paths
base_source_path <- "/Users/lewiswhite/CHAP_columbia/QuantAQ/data/sd_card_2024-09-01"  # Location of the downloaded date folders
base_destination_path <- "/Users/lewiswhite/CHAP_columbia/QuantAQ/data/sd_monitor_folders"  # Monitor folders you already created

# Ensure monitor folders exist or create them
for (monitor in monitor_names) {
  monitor_folder <- file.path(base_destination_path, monitor)
  if (!dir.exists(monitor_folder)) {
    dir.create(monitor_folder, recursive = TRUE)
    message(paste("Created monitor folder:", monitor_folder))
  }
}

# Get a list of all date folders in the base source path
date_folders <- list.dirs(base_source_path, full.names = TRUE, recursive = FALSE)

# Loop through monitor names
for (monitor in monitor_names) {
  # Replace all '-' with '_' to match the folder structure (correcting folder format)
  monitor_id <- str_replace_all(monitor, "-", "_")
  
  message(paste("Looking for monitor:", monitor_id))

  # Find all matching folders across all date directories
  matching_folders <- c()
  
  for (date_folder in date_folders) {
    # Find all the source folders that contain the monitor ID in the current date folder
    current_matching_folders <- list.dirs(date_folder, full.names = TRUE, recursive = TRUE)
    current_matching_folders <- current_matching_folders[grepl(monitor_id, current_matching_folders)]
    
    # Append any matches found
    if (length(current_matching_folders) > 0) {
      matching_folders <- c(matching_folders, current_matching_folders)
    }
  }
  
  # Debug: print the matching folders to check if they're correct
  if (length(matching_folders) > 0) {
    message(paste("Monitor:", monitor_id, " - Found matching folders:", paste(matching_folders, collapse = ", ")))
  } else {
    warning(paste("No matching folders found for monitor:", monitor_id))
  }
  
  # Move or copy each matching folder into its corresponding monitor directory
  for (folder in matching_folders) {
    destination_folder <- file.path(base_destination_path, monitor)
    destination <- file.path(destination_folder, basename(folder))
    
    # Copy the folder but check if files already exist, and avoid overwriting
    if (dir.exists(folder)) {
      tryCatch({
        dir_copy(folder, destination, overwrite = FALSE)  # Avoid overwriting existing files
        message(paste("Copied", folder, "to", destination))
      }, error = function(e) {
        message(paste("Error copying", folder, "to", destination, ":", e$message))
      })
    } else {
      warning(paste("Source folder", folder, "does not exist"))
    }
  }
}


```

